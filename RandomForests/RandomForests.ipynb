{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "602c3f40",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #ccffcc; padding: 10px;\">\n",
    "    <h1> Tutorial 3 </h1> \n",
    "    <h2> Random Forests </h2>\n",
    "</div>    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de39641",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "This tutorial is based on work done by Chetan Deva on Using random forest to predict leaf temperature from a number of measurable features.\n",
    "\n",
    "Plants regulate temp in extreme environments. e.g. a plant in a desert can stay 18C cooler than air temp or 22 C warmer than air in mountains. Leaf temperature differs from air temperature. Plant growth and development is strongly dependent on leaf temperature. Most Land Surface Models (LSMs) & Crop growth models (CGMs) use air temperature as an approximation of leaf temperature.\n",
    "\n",
    "However, during time periods when large differences exist, this can be an important source of input data uncertainty.\n",
    "\n",
    "In this tutorial leaf data containing a number of features is fed into a random forest regression model to evaluate which features are the most important to accurately predict the leaf temperature differential. The background scientific paper for the science behind the Leaf tempurature of which this tutorial is based is [Still et al 2019](https://esajournals.onlinelibrary.wiley.com/doi/pdf/10.1002/ecs2.2768)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72e8e18",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e6ccff; padding: 10px;\">\n",
    "\n",
    "\n",
    "<h1>Random Forests </h1>\n",
    "\n",
    "    \n",
    "Random forests are ensembles of decision trees where each decision tree produces a prediction and an average is taken, in this tutorial we will first build a simple decision tree and then build on that use python libraries to easily create Random Forest models to investigate what features are important in determining leaf temperature \n",
    "    \n",
    "## Recommended reading \n",
    "\n",
    "* [Random Forest overview linked with python](https://towardsdatascience.com/an-implementation-and-explanation-of-the-random-forest-in-python-77bf308a9b76)\n",
    "* [Random Forests Computer Science overview paper](https://link.springer.com/article/10.1023/A:1010933404324)\n",
    "\n",
    "\n",
    "<hr>\n",
    "    \n",
    "\n",
    "\n",
    "# The very basics\n",
    "    \n",
    "If you know nothing about machine learning and are finding the above links rather dry you might find the following youtube videos useful:\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3da8cfb",
   "metadata": {},
   "source": [
    "    \n",
    "<div style=\"background-color: #e6ccff; padding: 10px;\">      \n",
    "\n",
    "# Decision Trees\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad1c273",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #cce5ff; padding: 10px;\">\n",
    "\n",
    "*the cells below use ipython magics to embed youtube videos*\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a06695",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%HTML\n",
    "\n",
    "\n",
    "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/kakLu2is3ds\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9af874",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e6ccff; padding: 10px;\">\n",
    "    \n",
    "# Random Forests\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3bf9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%HTML\n",
    "\n",
    "\n",
    "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/v6VJ2RO66Ag\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7a466f",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #cce5ff; padding: 10px;\">\n",
    "\n",
    "<h1> Python </h1>\n",
    "\n",
    "Basic python knowledge is assumed for this tutorial and this tutorial will use [SciKit-Learn](https://scikit-learn.org/stable/) which covers a wide variety of useful machine learning tools. All the following code should run quickly on a standard laptop.\n",
    "    \n",
    "</div>\n",
    "    \n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2ddd8b",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #ffffcc; padding: 10px;\">\n",
    "    \n",
    "<h1> Requirements </h1>\n",
    "\n",
    "These notebooks should run with the following requirements satisfied\n",
    "\n",
    "<h2> Python Packages: </h2>\n",
    "\n",
    "* Python 3\n",
    "* scikit-learn\n",
    "* notebook\n",
    "* numpy\n",
    "* seaborn\n",
    "* matplotlib\n",
    "* pandas\n",
    "* statistics\n",
    "\n",
    "\n",
    "<h2> Data Requirements</h2>\n",
    "    \n",
    "This notebook referes to some data included in the git hub repositroy\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc145f1c",
   "metadata": {},
   "source": [
    "\n",
    "**Contents:**\n",
    "\n",
    "\n",
    "1. [Leaf Data](#Leaf-Data)\n",
    "2. [Decision Trees](#Decision-Trees)\n",
    "3. [Random Forests](#Random-Forests)\n",
    "4. [Hyper Parameters](#HyperParameters)\n",
    "5. [Using Automated Hyperparamter Selection](#Using-Automated-Hyperparamter-Selection)) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96af4e5",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #cce5ff; padding: 10px;\">\n",
    "Load in all required modules (includig some auxillary code) and turn off warnings. Make sure Keras session is clear\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085d7bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For readability: disable warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49573b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statistics\n",
    "import itertools\n",
    "from subprocess import call\n",
    "from IPython.display import Image\n",
    "# plotting libraries\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "# sklearn libraries\n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split, RepeatedKFold, cross_val_score\n",
    "from sklearn.metrics import precision_score, recall_score, roc_auc_score, roc_curve\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import confusion_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465fe7d3",
   "metadata": {},
   "source": [
    "# Leaf Data\n",
    "\n",
    "<div style=\"background-color: #ccffcc; padding: 10px;\">\n",
    " \n",
    "\n",
    "805 observations of bean leaves were taken over 3 separate growing seasons with adequate water and nitrogen (e.g. good conditions). \n",
    " \n",
    "<a href=\"https://esajournals.onlinelibrary.wiley.com/doi/pdf/10.1002/ecs2.2768\">\n",
    "<img src=\"images/leaf_engery_balance.png\" alt=\"Still et al 2019\"></a>\n",
    "    \n",
    "The leaf energy balance depends on shortwave radiation in ($Sw_{in}$),  Net longwave ($Lw_{in}$ vs. $Lw_{out}$)  and the cooling effects of leaf transpiration ($LE$)\n",
    "\n",
    "\n",
    "This is used to select features used (shown in the table below)\n",
    "    \n",
    "    \n",
    "| Feature    | Description |\n",
    "| :--------- | :---------- |\n",
    "| Air temperature  | 2 cm above the leaf  |\n",
    "| Leaf temperature | Using a contactless infrared camera  |\n",
    "| Relative Humidity  | Relative Humidity next to the leaf  |\n",
    "| Photosynthetically active radiation | The part of the spectrum the plant uses |\n",
    "| Photosynthetic Efficiency | % of incoming light going to photochemistry |\n",
    "| Proton conductivity  |  Steady state rate of proton flux across membrane |\n",
    "| Relative Chlorophyll  | Greenness of the leaf |\n",
    "| Leaf Thickness | Greenness of the leaf |\n",
    "| Leaf Angle | The leaf angle |\n",
    "\n",
    "    \n",
    "</div>\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055f97c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% import the data into a pandas dataframe\n",
    "\n",
    "df = pd.read_csv('data/df_prepped.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e409efce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print some information about the data\n",
    "\n",
    "print('There are '+ str(len(df)) + ' data entries\\n')\n",
    "print('Min Ambient Temperature is ' + str(df['Ambient Temperature'].values.min())+'\\n')\n",
    "print('Max Ambient Temperature is ' + str(df['Ambient Temperature'].values.max())+'\\n')\n",
    "print('Std Ambient Temperature is ' + str(df['Ambient Temperature'].values.std())+'\\n')\n",
    "print('Min Ambient Humidity is ' + str(df['Ambient Humidity'].values.min())+'\\n')\n",
    "print('Max Ambient Humidity is ' + str(df['Ambient Humidity'].values.max())+'\\n')\n",
    "print('Std Ambient Humidity is ' + str(df['Ambient Humidity'].values.std())+'\\n')\n",
    "print('Min Leaf Temperature Differential is ' + str(df['Leaf Temperature Differential'].values.min())+'\\n')\n",
    "print('Max Leaf Temperature Differential is ' + str(df['Leaf Temperature Differential'].values.max())+'\\n')\n",
    "print('Mean Leaf Temperature Differential is ' + str(df['Leaf Temperature Differential'].values.mean())+'\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dece9678",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #ccffcc; padding: 10px;\">\n",
    "\n",
    "## Summary of weather conditions in the data\n",
    "\n",
    "The data is in a fairly narrow range of temperature and relative humidity, but these ranges are typical for this crop breeding station.\n",
    "\n",
    "\n",
    "## Leaf Temperature Differential \n",
    "\n",
    "The mean difference between leaf and air temperature is about -4 (°C) meaning the leaves are often cooler than the air .\n",
    "    \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35207ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% calculate required variables \n",
    "\n",
    "# leaf temperature \n",
    "df['ltemp'] = df['Ambient Temperature'] + df['Leaf Temperature Differential']\n",
    "\n",
    "#%% define the target variable and the predictors\n",
    "\n",
    "# define the variable for prediction\n",
    "y = df['ltemp']\n",
    "\n",
    "# define the dataframe of predictor variables \n",
    "X = df.drop(['ltemp', 'Leaf Temperature Differential'], axis = 1)\n",
    "\n",
    "# create list of random numbers \n",
    "rnumbers = list(np.random.randint(1,1000, size=X.shape[0]))\n",
    "\n",
    "# create dataframe for feature selection purposes only including a column of random numbers\n",
    "X_features = X\n",
    "X_features['random_numbers'] = rnumbers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22cffa1f",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #ccffcc; padding: 10px;\">\n",
    "    \n",
    "\n",
    "# Decision Tree Simple Example\n",
    "\n",
    "First, we create the features `X_2f` from a subset of our data and the labels `yy`. There are only two features, which will allow us to visualize the data and which makes this a very easy problem.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df1f0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed to ensure \"reproducible\" runs\n",
    "RSEED = 50\n",
    "# X_2f\n",
    "X_1f = np.zeros((805,2))\n",
    "X_1f[:,0] = np.array(X['Ambient Temperature'])\n",
    "X_1f[:,1] = np.array(X['Ambient Humidity'])\n",
    "N=800\n",
    "X_1f=X_1f[0:N,:]\n",
    "# label classes for simplicty split into two classes low or high\n",
    "yy=pd.DataFrame()\n",
    "yy['vals']=y[0:N]\n",
    "yy['label']=pd.qcut(yy.vals.values, 2, labels=[0, 1])\n",
    "yy['strlabel']=''\n",
    "yy.strlabel[yy.label==1]='high'\n",
    "yy.strlabel[yy.label==0]='low'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89cc3026",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #ccffcc; padding: 10px;\">\n",
    "    \n",
    "## Data Visualization\n",
    "\n",
    "To get a sense of the data, we can graph some of the data points with the number showing the label.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ed71ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot data?\n",
    "plt.style.use('fivethirtyeight')\n",
    "plt.rcParams['font.size'] = 18\n",
    "plt.figure(figsize = (12, 12))\n",
    "\n",
    "# Plot a subset each point as the label\n",
    "for x1, x2, label in zip(X_1f[0:20, 0], X_1f[0:20, 1], yy.strlabel[0:20].values):\n",
    "    plt.text(x1, x2, label, fontsize = 34, color = 'g',\n",
    "             ha='center', va='center')\n",
    "    \n",
    "# Plot formatting\n",
    "plt.grid(None);\n",
    "plt.xlim((30, 34));\n",
    "plt.ylim((47, 64));\n",
    "plt.xlabel('air temp', size = 20); plt.ylabel('humidity', size = 20); plt.title('Leaf Temp', size = 24)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381d7f93",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #ccffcc; padding: 10px;\">\n",
    "    \n",
    "This shows a simple linear classifier will not be able to draw a boundary that separates the classes. The single decision tree will be able to completely separate the points because it essentially draws many repeated linear boundaries between points. A decision tree is a non-parametric model because the number of parameters grows with the size of the data.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c219919a",
   "metadata": {},
   "source": [
    "# Decision Trees\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da2ea9a",
   "metadata": {},
   "source": [
    "\n",
    "<div style=\"background-color: #cce5ff; padding: 10px;\">\n",
    "\n",
    "Here we quickly build and train a single decision tree on the data using Scikit-Learn `sklearn.DecisionTreeClassifier`. The tree will learn how to separate the points, building a flowchart of questions based on the feature values and the labels. At each stage, the decision tree splits by maximizing the reduction in Gini impurity.\n",
    "\n",
    "    \n",
    "We'll use the default hyperparameters for the decision tree which means it can grow as deep as necessary in order to completely separate the classes. This will lead to overfitting because the model memorizes the training data, and in practice, we usually want to limit the depth of the tree so it can generalize to testing data.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ee9d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a decision tree and train\n",
    "tree = DecisionTreeClassifier(random_state=RSEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78be1f7a",
   "metadata": {},
   "source": [
    "# Altering max depth\n",
    "\n",
    "<div style=\"background-color: #cce5ff; padding: 10px;\">\n",
    "\n",
    "Once you have ran through the next few cells you will see a link back to this cell to investigate the impact of altering the tree depth which you can do by uncommenting the cell below\n",
    "    \n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eddf8357",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After runnning the follow cells try seeing the effect of limiting the max depth\n",
    "# uncomment below code - you can play around with altering max depth to see the limits\n",
    "\n",
    "# tree = DecisionTreeClassifier(max_depth=4, random_state=RSEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc408b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.fit(X_1f, yy.label.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced4b9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Decision tree has {tree.tree_.node_count} nodes with maximum depth {tree.tree_.max_depth}.')\n",
    "print(f'Model Accuracy: {tree.score(X_1f, yy.label.values )}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1aecfb",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #ccffcc; padding: 10px;\">\n",
    "\n",
    "Without limitting the the depth of the tree the model will have achieved 100% accuracy \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b0b725",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 30% examples in test data\n",
    "train, test, train_labels, test_labels = train_test_split(X_1f,yy.label.values , \n",
    "                                                          stratify = yy.label.values,\n",
    "                                                          test_size = 0.3, \n",
    "                                                          random_state = RSEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf27f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features for feature importances\n",
    "features = ['Ambient Temperature','Ambient Humidity']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e882b8cb",
   "metadata": {},
   "source": [
    "## Visualizing the decision tree\n",
    "\n",
    "<div style=\"background-color: #ccffcc; padding: 10px;\">\n",
    "To get a sense of how the decision tree \"thinks\", it's helpful to visualize the entire structure. This will show each node in the tree which we can use to make new predictions. Because the tree is relatively small, we can understand the entire image.\n",
    "\n",
    "</div>\n",
    "<div style=\"background-color: #cce5ff; padding: 10px;\">\n",
    "    \n",
    "We can use `graphviz` to visualise and limit the depth so the tree isn't too large to display \n",
    "\n",
    "**Note graphviz dependency doens't always work out the box so will load example image if this fails**\n",
    "    \n",
    "`export_graphviz` takes the `max_depth=n` parameter you may wish to remove this in the commented out command below to see how crazy the the tree now looks (may take a bit longer to produce!)   \n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f373be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save tree as dot file\n",
    "export_graphviz(tree, 'tree_example.dot', rounded = True, \n",
    "                feature_names = features, max_depth=6,\n",
    "                class_names = ['high leaf temp', 'low leaf temp'], filled = True)\n",
    "\n",
    "# Unlimitted graphviz\n",
    "# export_graphviz(tree, 'tree_example.dot', rounded = True, feature_names = features,  class_names = ['high leaf temp', 'low leaf temp'], filled = True)\n",
    "\n",
    "# Convert to png\n",
    "try:\n",
    "    call(['dot', '-Tpng', 'tree_example.dot', '-o', 'tree_example.png', '-Gdpi=200'])\n",
    "except:\n",
    "    print('graphviz failed')\n",
    "    print('Loaded the max depth limited figure pre-produced incase of graphviz failure \\n')\n",
    "    print('If you would like to see the results having limited the max depth please uncomment the last line in this cell')\n",
    "\n",
    "    \n",
    "Image(filename='tree_example.png')\n",
    "#Image(filename='tree_example_max_depth_4.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236db835",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #ccffcc; padding: 10px;\">\n",
    "    \n",
    "At each node the decision tree considers a feature based question reducing the Gini impurity\n",
    "\n",
    "\n",
    "### Gini Impurity \n",
    "\n",
    "The probability  that a randomly selected sample from a node will be incorrectly classified according to the distribution of samples in a node. At each split the tree tries to pick values that reduce the gini impurity, if max depth is not limited we get to 0 for every training point as no limit was set (full tree not shown here)\n",
    "\n",
    "<hr>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca157ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train tree\n",
    "tree.fit(train, train_labels)\n",
    "print(f'Decision tree has {tree.tree_.node_count} nodes with maximum depth {tree.tree_.max_depth}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4adf9d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Will over fit\n",
    "# Make probability predictions\n",
    "train_probs = tree.predict_proba(train)[:, 1]\n",
    "probs = tree.predict_proba(test)[:, 1]\n",
    "\n",
    "train_predictions = tree.predict(train)\n",
    "predictions = tree.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad71119",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Train ROC AUC Score: {roc_auc_score(train_labels, train_probs)}')\n",
    "print(f'Test ROC AUC  Score: {roc_auc_score(test_labels, probs)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4580ccf2",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #ccffcc; padding: 10px;\">\n",
    "    \n",
    "# Evaluating the mode\n",
    "\n",
    "[Receiver Operating Characteristic (ROC) curves](https://medium.com/cascade-bio-blog/making-sense-of-real-world-data-roc-curves-and-when-to-use-them-90a17e6d1db) describe the trade-off between the true positive rate (TPR) and false positive (FPR) rate along different probability thresholds for a classifier. \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1565b61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(predictions, probs, train_predictions, train_probs):\n",
    "    \"\"\"Compare machine learning model to baseline performance.\n",
    "    Computes statistics and shows ROC curve.\"\"\"\n",
    "    \n",
    "    baseline = {}\n",
    "    \n",
    "    baseline['recall'] = recall_score(test_labels, [1 for _ in range(len(test_labels))])\n",
    "    baseline['precision'] = precision_score(test_labels, [1 for _ in range(len(test_labels))])\n",
    "    baseline['roc'] = 0.5\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    results['recall'] = recall_score(test_labels, predictions)\n",
    "    results['precision'] = precision_score(test_labels, predictions)\n",
    "    results['roc'] = roc_auc_score(test_labels, probs)\n",
    "    \n",
    "    train_results = {}\n",
    "    train_results['recall'] = recall_score(train_labels, train_predictions)\n",
    "    train_results['precision'] = precision_score(train_labels, train_predictions)\n",
    "    train_results['roc'] = roc_auc_score(train_labels, train_probs)\n",
    "    \n",
    "    for metric in ['recall', 'precision', 'roc']:\n",
    "        print(f'{metric.capitalize()} Baseline: {round(baseline[metric], 2)} Test: {round(results[metric], 2)} Train: {round(train_results[metric], 2)}')\n",
    "    \n",
    "    # Calculate false positive rates and true positive rates\n",
    "    base_fpr, base_tpr, _ = roc_curve(test_labels, [1 for _ in range(len(test_labels))])\n",
    "    model_fpr, model_tpr, _ = roc_curve(test_labels, probs)\n",
    "\n",
    "    plt.figure(figsize = (8, 6))\n",
    "    plt.rcParams['font.size'] = 16\n",
    "    \n",
    "    # Plot both curves\n",
    "    plt.plot(base_fpr, base_tpr, 'b', label = 'baseline')\n",
    "    plt.plot(model_fpr, model_tpr, 'r', label = 'model')\n",
    "    plt.legend();\n",
    "    plt.xlabel('False Positive Rate'); plt.ylabel('True Positive Rate'); plt.title('ROC Curves');\n",
    "evaluate_model(predictions, probs, train_predictions, train_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef87990",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #ccffcc; padding: 10px;\">\n",
    "\n",
    "If the model line is to the left of the blue it is performing better than random chance     \n",
    "    \n",
    "</div>\n",
    "<div style=\"background-color: #ccffcc; padding: 10px;\">\n",
    "    \n",
    "## Feature Importances\n",
    "\n",
    "We can extract the features considered most important by the Decision Tree. The values are computed by summing the reduction in Gini Impurity over all of the nodes of the tree in which the feature is used, below shows the relative importance assigned to Ambient Temperature vs Ambient humidity    \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a796af",
   "metadata": {},
   "outputs": [],
   "source": [
    "fi = pd.DataFrame({'feature': features,\n",
    "                   'importance': tree.feature_importances_}).\\\n",
    "                    sort_values('importance', ascending = False)\n",
    "fi.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb055b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Oranges):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    Source: http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.figure(figsize = (10, 10))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title, size = 24)\n",
    "    plt.colorbar(aspect=4)\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45, size = 14)\n",
    "    plt.yticks(tick_marks, classes, size = 14)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    \n",
    "    # Labeling the plot\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt), fontsize = 20,\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "        \n",
    "    plt.grid(None)\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label', size = 18)\n",
    "    plt.xlabel('Predicted label', size = 18)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba40e437",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #cce5ff; padding: 10px;\">\n",
    "\n",
    "The function `plot_confusion_matrix` has the ability to normalise the values which may aid in interpretation - you can see this by uncommenting the last line in the cell below\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856227fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(test_labels, predictions)\n",
    "plot_confusion_matrix(cm, classes = ['High Leaf Temp', 'Low Leaf Temp'],\n",
    "                      title = 'Leaf Temp Confusion Matrix')\n",
    "\n",
    "# plot_confusion_matrix(cm,  normalize=True, classes = ['High Leaf Temp', 'Low Leaf Temp'], title = 'Leaf Temp Confusion Matrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0adfc5c8",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e6ccff; padding: 10px;\">\n",
    "    \n",
    "Limit Maximum Depth\n",
    "\n",
    "In practice, we usually want to limit the maximum depth of the decision tree (even in a random forest) so the tree can generalize better to testing data. Although this will lead to reduced accuracy on the training data, it can improve performance on the testing data.\n",
    "\n",
    "</div>\n",
    "<div style=\"background-color: #cce5ff; padding: 10px;\">\n",
    "    \n",
    "**Try going back to [Altering Max Depth](#Altering-max-depth) and uncommenting the line:** `tree = DecisionTreeClassifier(max_depth=4, random_state=RSEED)`\n",
    "    \n",
    "**and re-running the rest of the following cells until this one**\n",
    "\n",
    "</div>\n",
    "<div style=\"background-color: #e6ccff; padding: 10px;\">\n",
    "    \n",
    "The model no longer gets perfect accuracy on the training data. However, it probably would do better on the testing data since we have limited the maximum depth to prevent overfitting. This is an example of the bias - variance tradeoff in machine learning. A model with high variance has learned the training data very well but often cannot generalize to new points in the test set. On the other hand, a model with high bias has not learned the training data very well because it does not have enough complexity. This model will also not perform well on new points.\n",
    "\n",
    "Limiting the depth of a single decision tree is one way we can try to make a less biased model. Another option is to use an entire forest of trees, training each one on a random subsample of the training data. The final model then takes an average of all the individual decision trees to arrive at a classification. This is the idea behind the random forest.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e8bf45",
   "metadata": {},
   "source": [
    "\n",
    "# Random Forests\n",
    "\n",
    "<div style=\"background-color: #ccffcc; padding: 10px;\">\n",
    "    \n",
    "An ensemble (1000 or 100,000 s) of decision trees, training each tree on a random set of observations and for each node only a subset of features are used and the predictions are averaged to arrive at the final classification\n",
    "\n",
    "\n",
    "</div>\n",
    "\n",
    "<div style=\"background-color: #cce5ff; padding: 10px;\">\n",
    "\n",
    "We're going to use the [scikit-learn RandomForestRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html) to set up our model \n",
    "    \n",
    "`RandomForestRegressor(max_features, random_state=SEED, n_estimators , max_depth)`\n",
    "\n",
    "This is a slight tweak to allow us to look at continuous values rather than discrete categories as outline in [this article](https://medium.com/swlh/random-forest-and-its-implementation-71824ced454f) if you wish to understand the difference.      \n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9173f3",
   "metadata": {},
   "source": [
    "# Random Forest Hyperparameters \n",
    "\n",
    "<div style=\"background-color: #e6ccff; padding: 10px;\">\n",
    "\n",
    "In our randomforest model defined in the python code below    \n",
    "    \n",
    "`RandomForestRegressor(max_features = 3, random_state=SEED, n_estimators = 100, max_depth = md)`\n",
    "    \n",
    "* **Max features `max_features`** The number of features to consider when looking for the best split. This could be set to N/3 as a quick heuristic for regression (rounding up).\n",
    "\n",
    "* **Max samples :** The proportion of the data set used for bootstrapping. The default is set to using the whole data set for bootstrapping, and is usually a sensible way to go so not passed into our function.\n",
    "\n",
    "* **Number of Trees `n_estimators`:** The number of trees grown in the forest. In general, more trees will result in better performance, which eventually plateaus. Over-fitting is not a danger here.\n",
    "\n",
    "* **Max depth `max_depth`:** The depth of a decision tree. The default is to keep trees unpruned.\n",
    "    \n",
    "**NB** `random_state=SEED` is set to make runs reproducible\n",
    "\n",
    "\n",
    "\n",
    "# [K-fold cross validation](https://towardsdatascience.com/k-fold-cross-validation-explained-in-plain-english-659e33c0bc0)\n",
    "\n",
    "<img src=\"images/kfold.png\">\n",
    "\n",
    "1. Split the data into k folds \n",
    "2. Reserve 1 fold for testing and use the remaining n-1 folds for training \n",
    "3. Repeat the procedure over all k folds\n",
    "4. Average performance evaluation statistics across folds. \n",
    "\n",
    "</div>\n",
    "\n",
    "<div style=\"background-color: #cce5ff; padding: 10px;\">\n",
    "In this piece of work I use 10 Folds and repeat the whole process 3 times.\n",
    "\n",
    "`RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)`\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "<div style=\"background-color: #e6ccff; padding: 10px;\">\n",
    "\n",
    "\n",
    "# Evaluating the Random Forest \n",
    "\n",
    "* [R-squared](https://en.wikipedia.org/wiki/Coefficient_of_determination) : proportion of variance explained by the model \n",
    "\n",
    "* [RMSE](https://en.wikipedia.org/wiki/Mean_squared_error): standard deviation of the prediction errors (residuals)\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3d4bcf",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #cce5ff; padding: 10px;\">\n",
    "\n",
    "The sensitivity steps might take a few mins to run depending on computure performance\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759c6d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conduct sensitivity test on max depth\n",
    "\n",
    "# seed \n",
    "SEED = 1\n",
    "\n",
    "# range of tree depths\n",
    "mds = np.arange(1,30)\n",
    "\n",
    "# list for evaluation\n",
    "r2_sens_lst_md = []\n",
    "rmse_sens_lst_md = []\n",
    "\n",
    "# loop over all values for maximum depth between 1 and 30, storing the r2 and rmse for 10 fold cross validation\n",
    "for i in range(len(mds)):\n",
    "    md = mds[i]\n",
    "    rf_sens = RandomForestRegressor(max_features = 3, random_state=SEED, n_estimators = 100, max_depth = md)\n",
    "    cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "    r2s_sens = list(cross_val_score(rf_sens, X, y, scoring='r2', cv=cv, n_jobs=-1, error_score='raise'))\n",
    "    rmses_sens = list(cross_val_score(rf_sens, X, y, scoring='neg_root_mean_squared_error', cv=cv, n_jobs=-1, error_score='raise'))\n",
    "    r2_sens = statistics.mean(r2s_sens)\n",
    "    rmse_sens = statistics.mean(rmses_sens)\n",
    "    r2_sens_lst_md.append(r2_sens)\n",
    "    rmse_sens_lst_md.append(rmse_sens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33c100a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# max depth dataframe \n",
    "df_sens_md = pd.DataFrame({'max_depth' : list(mds), 'r2' : r2_sens_lst_md, 'rmse' : rmse_sens_lst_md})\n",
    "df_sens_md['rmse'] = df_sens_md['rmse']*-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dccb222b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatterplot for maximum depth vs.performance metric (change 'r2' for 'rmse' to replicate the plots in the presentation)\n",
    "sns.scatterplot(x = 'max_depth', y = 'r2', data = df_sens_md)\n",
    "plt.title('max depth vs. r2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5159117d",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #ccffcc; padding: 10px;\">\n",
    "\n",
    "## Sensitivity Testing for Maximum Depth \n",
    "    \n",
    "Past a maximum tree depth of approx. 25, there is no change to either the variance explained or the bias.\n",
    "For this reason, it is useful to prune the depth of the random forest to this value to save computation time.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9cfcf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% conduct sensitivity test on number of parameters to split\n",
    "\n",
    "# range of parameters\n",
    "mxf = np.arange(1,9)\n",
    "\n",
    "# list for evaluation\n",
    "r2_sens_lst_mxf = []\n",
    "rmse_sens_lst_mxf = []\n",
    "\n",
    "# loop over all values for maximum number of features used in splitting between 1 and 9, storing the r2 and rmse for 10 fold cross validation\n",
    "for i in range(len(mxf)):\n",
    "    mx = mxf[i]\n",
    "    rf_sens = RandomForestRegressor(max_features = mx, random_state=SEED, n_estimators = 100)\n",
    "    cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "    r2s_sens = list(cross_val_score(rf_sens, X, y, scoring='r2', cv=cv, n_jobs=-1, error_score='raise'))\n",
    "    rmses_sens = list(cross_val_score(rf_sens, X, y, scoring='neg_root_mean_squared_error', cv=cv, n_jobs=-1, error_score='raise'))\n",
    "    r2_sens = statistics.mean(r2s_sens)\n",
    "    rmse_sens = statistics.mean(rmses_sens)\n",
    "    r2_sens_lst_mxf.append(r2_sens)\n",
    "    rmse_sens_lst_mxf.append(rmse_sens)\n",
    "\n",
    "# max depth dataframe \n",
    "df_sens_mxf = pd.DataFrame({'max_features' : list(mxf), 'r2' : r2_sens_lst_mxf, 'rmse' : rmse_sens_lst_mxf})\n",
    "df_sens_mxf['rmse'] = df_sens_mxf['rmse']*-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5d6dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatterplot for maximum number of features vs.performance metric (change 'r2' for 'rmse' to replicate the plots in the presentation)\n",
    "sns.scatterplot(x = 'max_features', y = 'r2', data = df_sens_mxf)\n",
    "plt.title('max features vs. r2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6293058",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #ccffcc; padding: 10px;\">\n",
    "    \n",
    "## Sensitivity Testing for Maximum Features in Splits\n",
    "\n",
    "Past a maximum features of 5, there is very little increase in variance explained or improvement in model bias. Therefore Max features of 5 is selected. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6505dd55",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Evaluate the performance of the Random Forest Algorithm with chosen parameters from the sensitivity analysis using cross validation \n",
    "\n",
    "# instantiate the random forest regressor\n",
    "rf = RandomForestRegressor(max_features = 5, random_state=SEED, n_estimators = 100, max_depth = 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e5f451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the cross validation\n",
    "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d23eee1",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #cce5ff; padding: 10px;\">\n",
    "\n",
    "**a note on the `scoring='neg_root_mean_squared_error'` parameter**\n",
    "\n",
    "The keen eyed will spot something odd with the idea of a negative square root! The reason for this negative sign is because cross_val_score() reports scores in ascending order (largest score is best). But RMSE is naturally descending scores (the smallest score is best). Thus we need to use ‘neg_mean_squared_error’ to invert the sorting. This also results in the score to be negative even though the value can never be negative.\n",
    "\n",
    "The actual RMSE is simply -1 x NegRMSE\n",
    "  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aab21b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the metrics for each kfold cross validation\n",
    "r2s = list(cross_val_score(rf, X, y, scoring='r2', cv=cv, n_jobs=-1, error_score='raise'))\n",
    "rmses = list(cross_val_score(rf, X, y, scoring='neg_root_mean_squared_error', cv=cv, n_jobs=-1, error_score='raise'))\n",
    "\n",
    "# take the mean of these \n",
    "r2 = statistics.mean(r2s)\n",
    "rmse = statistics.mean(rmses)\n",
    "\n",
    "print('mean r2 = '+ str(r2))\n",
    "print('mean rmse = '+ str(-1*rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a094f1",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #ccffcc; padding: 10px;\">\n",
    "\n",
    "# Predictive ability \n",
    "    \n",
    " The model is able to explain a useful share of variance explained (mean = 0.77) with a reasonably low bias  (mean = 1.50)\n",
    "\n",
    "This suggests that expanded versions of this model may be a useful sub-module in land surface / crop growth models.\n",
    "\n",
    "\n",
    "# Feature importance\n",
    "\n",
    "Clearly Ambient Temperature and Ambient Humidity are the most important features for prediction. However, features like photosynthetic efficiency, membrane conductance and relative chlorophyll also contributed. Their importance was a lot greater than a column of random numbers. Many of these quantities are available from satellites, so this kind of model may be applicable across spatial scales.\n",
    "    \n",
    "<div style=\"background-color: #ccffcc; padding: 10px;\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1265d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% calculate the feature importances \n",
    "\n",
    "# fit the random forest regressor with the dataframe of predictor variables that includes the column of random numbers for comparison\n",
    "rf.fit(X_features, y)\n",
    "\n",
    "# feature importances\n",
    "importance = rf.feature_importances_\n",
    "\n",
    "# features\n",
    "features = list(X_features.columns)\n",
    "\n",
    "# dataframe for plotting \n",
    "df_features = pd.DataFrame(list(zip(importance, features)), \n",
    "               columns =['importance', 'features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68abb67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% plot the feature importances\n",
    "\n",
    "sns.barplot(x = 'importance', y = 'features', data = df_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40671aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% compose the statistics for each cross validation effort in a dataframe for plotting.\n",
    "\n",
    "df_eval = pd.DataFrame({'r2' : r2s, 'rmse' : rmses})\n",
    "df_eval['rmse'] = df_eval['rmse']*-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f235e46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% plot the statistics\n",
    "\n",
    "# replace r2 with rmse to reproduce the plots in the presentation\n",
    "sns.boxplot(x ='r2', data = df_eval)\n",
    "plt.title('10 fold cross validation repeated 3 times')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe9316e",
   "metadata": {},
   "source": [
    "# Using Automated Hyperparamter Selection\n",
    "\n",
    "<div style=\"background-color: #cce5ff; padding: 10px;\">\n",
    "\n",
    "We can use `RandomizedSearchCV` to search a parameter grid to search for what hyparameters will create the best perfroming model\n",
    "    \n",
    "* increasing `n_iter` may increase performace but will be slower to run\n",
    "* `cv = 10` is a 10 fold cross validation\n",
    "* `scoring = 'r2'` scores based on r squared   \n",
    "\n",
    "more information on this can be found [here](https://scikit-learn.org/stable/modules/grid_search.html)\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a4867f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimator for use in random search\n",
    "estimator = RandomForestRegressor( random_state=SEED)\n",
    "\n",
    "# Hyperparameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': np.linspace(10, 200).astype(int),\n",
    "    'max_depth': [None] + list(np.linspace(3, 20).astype(int)),\n",
    "    'max_features': ['auto', 'sqrt', None] + list(np.arange(0.5, 1, 0.1)),\n",
    "    'max_leaf_nodes': [None] + list(np.linspace(10, 50, 500).astype(int)),\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create the random search model\n",
    "rs = RandomizedSearchCV(estimator, param_grid, n_jobs = -1, \n",
    "                        scoring = 'r2', cv = 10, \n",
    "                        n_iter = 50, verbose = 1, random_state=RSEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3641cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit \n",
    "rs.fit(train, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf5ad26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Took several mins \n",
    "# Now we can access the best hyper parameteres\n",
    "rs.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9a1166",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #ccffcc; padding: 10px;\">\n",
    "    \n",
    "This produces slightly different hyperparameters that our sensitivity testing\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2ea4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = rs.best_estimator_\n",
    "# calculate the metrics for each kfold cross validation\n",
    "r2s = list(cross_val_score(best_model, X, y, scoring='r2', cv=cv, n_jobs=-1, error_score='raise'))\n",
    "rmses = list(cross_val_score(best_model, X, y, scoring='neg_root_mean_squared_error', cv=cv, n_jobs=-1, error_score='raise'))\n",
    "\n",
    "# take the mean of these \n",
    "r2 = statistics.mean(r2s)\n",
    "rmse = statistics.mean(rmses)\n",
    "\n",
    "print('mean r2 = '+ str(r2))\n",
    "print('mean rmse = '+ str(rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564fbe04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% calculate the feature importances \n",
    "\n",
    "# fit the random forest regressor with the dataframe of predictor variables that includes the column of random numbers for comparison\n",
    "best_model.fit(X_features, y)\n",
    "\n",
    "# feature importances\n",
    "importance = best_model.feature_importances_\n",
    "\n",
    "# features\n",
    "features = list(X_features.columns)\n",
    "\n",
    "# dataframe for plotting \n",
    "df_features = pd.DataFrame(list(zip(importance, features)), \n",
    "               columns =['importance', 'features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c73d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% plot the feature importances\n",
    "\n",
    "sns.barplot(x = 'importance', y = 'features', data = df_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e811d4b",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #ccffcc; padding: 10px;\">\n",
    "\n",
    "The best model selected here uses less features but the broad pattern remains the same\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8af040",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% compose the statistics for each cross validation effort in a dataframe for plotting.\n",
    "\n",
    "df_eval = pd.DataFrame({'r2' : r2s, 'rmse' : rmses})\n",
    "df_eval['rmse'] = df_eval['rmse']*-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abaedd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% plot the statistics\n",
    "\n",
    "# replace r2 with rmse to reproduce the plots in the presentation\n",
    "sns.boxplot(x ='r2', data = df_eval)\n",
    "plt.title('10 fold cross validation repeated 3 times')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feffadb7",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #ccffcc; padding: 10px;\">\n",
    "\n",
    "The best model here performs slightly worse than our sensitivy testing hyperparameter selection. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a5dc1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
