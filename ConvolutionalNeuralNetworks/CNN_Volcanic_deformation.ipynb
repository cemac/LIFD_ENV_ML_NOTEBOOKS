{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5ecefaf",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #ccffcc; padding: 10px;\">\n",
    "    <h1> Tutorial 1 </h1> \n",
    "    <h2> Classification of Volcanic Deformation using Convolutional Neural Networks </h2>\n",
    "</div>    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3e2668",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This tutorial is based on work done by Matthew Gaddes and full code [VUDLNet_21](https://github.com/matthew-gaddes/VUDLNet_21)<sup>[1]</sup>. Creating a Convolutional Neural Network that will detect and localise deformation in Sentinel-1 Interferogram. A database of labelled Sentinel-1 data hosted at [VolcNet](https://github.com/matthew-gaddes/VolcNet) is used to train the CNN. \n",
    "\n",
    "\n",
    "A small subset of the data required is provided to create a tutorial that will both run in a short time frame and not be over file size limits set by GitHub. An option is included to download and use larger bottleneck files created from a larger data set to see a higher performance example. \n",
    "\n",
    "[1] [Matthew Gaddes, Andy Hooper , Fabien Albino 2021](https://eartharxiv.org/repository/view/1969/)\n",
    "\n",
    "### Summary for non voclanologists\n",
    "\n",
    "Interferograms are ground deformation maps produced by interferometric4synthetic aperture radar (InSAR). Specific deformation patterns are associated with different types of sources of deformation (volcanic activity). A trained person can, by eye, identify the source and location of deformation. However, when looking at a large number of interferograms automation is required.\n",
    "<a href=\"https://nisar.jpl.nasa.gov/mission/get-to-know-sar/interferometry/\">\n",
    "<img src=\"https://nisar.jpl.nasa.gov/rails/active_storage/blobs/eyJfcmFpbHMiOnsibWVzc2FnZSI6IkJBaHBMUT09IiwiZXhwIjpudWxsLCJwdXIiOiJibG9iX2lkIn19--a4d00ff0f922fe5307756184759187b5f14ae895/Interferometry1.jpg?disposition=attachment\" width=\"500\" height=\"600\" alt=\"https://nisar.jpl.nasa.gov/mission/get-to-know-sar/interferometry/\">\n",
    "</a>\n",
    "\n",
    "*taken from NASA JPL*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac11381",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #ccffcc; padding: 10px;\">\n",
    "\n",
    "<h1>Convolutional Neural Networks </h1>\n",
    "\n",
    "This tutorial will use Convolutional Neural Networks to classify volcanic deformation.\n",
    " \n",
    "## The very basics\n",
    "    \n",
    "If you know nothing about neural networks there is a [toy neural network python code example](https://github.com/cemac/LIFD_ENV_ML_NOTEBOOKS/tree/main/ToyNeuralNetwork) included in the [LIFD ENV ML Notebooks Repository]( https://github.com/cemac/LIFD_ENV_ML_NOTEBOOKS). Creating a 2 layer neural network to illustrate the fundamentals of how Neural Networks work and the equivlent code using the python machine learning library [keras](https://keras.io/). \n",
    "    \n",
    "## Recommended reading \n",
    "\n",
    "The in-depth theory behind convolution neural networks will not be covered here as this tutorial is focusing on how to use them for a certain earth science application. If you wish to learn more here are some great starting points.     \n",
    "    \n",
    "1. [The very basics in a Victor Zhou Blog](https://victorzhou.com/blog/intro-to-cnns-part-1/)\n",
    "2. [A deep dive into CNNs in towards data science](https://towardsdatascience.com/deep-dive-into-convolutional-networks-48db75969fdf)\n",
    "3. [More information on transfer learning (using pre-trained models)](https://machinelearningmastery.com/how-to-use-transfer-learning-when-developing-convolutional-neural-network-models/)\n",
    "4. [Section 5 of the example ipython notebooks from the fchollet deep learning with python repository](https://github.com/fchollet/deep-learning-with-python-notebooks)   \n",
    "    \n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b221e10",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "\n",
    "<div style=\"background-color: #e6ccff; padding: 10px;\">\n",
    "    \n",
    "<h1> Machine Learning Theory </h1>\n",
    "<a href=\"https://towardsdatascience.com/deep-dive-into-convolutional-networks-48db75969fdf\">\n",
    "<img src=\"https://miro.medium.com/max/2510/1*XbuW8WuRrAY5pC4t-9DZAQ.jpeg\">\n",
    "</a>\n",
    "\n",
    " To create a convolutional neural network that both classify the type of deformation and the location a \"two-headed model\" is built to return both from one forward pass of an interferogram through the network. As models have already been trained to identify features it is possible to transfer weights from other models designed for different problems. In this tutorial, the VGG16 model is used as this was found to be sensitive to the signals of interest in interferograms.  \n",
    "    \n",
    "    \n",
    "## Convolutional Neural Networks\n",
    "\n",
    "A Neural Network is essentially a mathematical function that maps a given input to the desired output by adjusting weights and biases over many layers. \n",
    "\n",
    "![https://victorzhou.com/series/neural-networks-from-scratch/](images/network.png)\n",
    "    \n",
    "**Convolutional Neural Networks** (CCNs) are a popular variant of neural networks used for image classification. CNNs do not require as many weights as a normal neural network, they use the fact that each pixels neighbour provides some context to allow for localised features to be detected. CNN's use [convolutions](https://en.wikipedia.org/wiki/Convolution) to create a set of filters for each layer to create an output image by convolving a set of filters with the input image to create an output volume.\n",
    "    \n",
    "CNNs go through a set of steps.\n",
    "    \n",
    "1. Convolve the input image with a set of filters to create output volumes\n",
    "2. Pool layers as much of the information contained in each layer's output is redundant as neighbouring pixels produce similar values. (This essentially reduces the output volume)\n",
    "3. Create a [softmax layer](https://victorzhou.com/blog/softmax/) fully connected (dense) layer to predict the outcome with the highest probability.  \n",
    "       \n",
    "    \n",
    "## VGG16 Model\n",
    "    \n",
    "Deep CCNs can take days or weeks to train if using very large datasets. This process can be shortened by re-using model weights from pre-trained models that were developed for standard computer vision benchmark datasets. i.e. some layers of a model trained on one problem that is similar to the actual problem that you're interested in can be used as a shortcut in training your model.  \n",
    "    \n",
    "The VGG16 model was developed by the [Visual Graphics Group (VGG) at Oxford](https://arxiv.org/abs/1409.1556) and works on recognising consistent and repeating structures. Here we will use 5 convolutional blocks of VGG16 on the input data before using a home built fully connected CNN. VGG16 is easily accessible via `Keras`.  \n",
    "    \n",
    "  \n",
    "    \n",
    "</div>    \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b58966b",
   "metadata": {},
   "source": [
    "  \n",
    "<div style=\"background-color: #cce5ff; padding: 10px;\">\n",
    "\n",
    "<h1> Python </h1>\n",
    "\n",
    "Basic python knowledge is assumed for this tutorial. A number of complex data processing and visualisation functions have been written and stored in aux_functions.py (code taken from [VUDLNet_21](https://github.com/matthew-gaddes/VUDLNet_21)). For this tutorial the main machine learning library we'll be working [Keras](https://keras.io/). Python specific information will be colour coded blue.\n",
    " \n",
    "    \n",
    "## Keras\n",
    "    \n",
    "There are many machine learning python libraries available, Keras is one such library although it now comes bundled with [TensorFlow](https://www.tensorflow.org/) and the recommendation is to use TensorFlow's Keras API instead of the standalone `Keras` package. Keras is a high-level API designed to make using a more low-level library like TensorFlow easier and more intuitive to use so you don't need to know about the hardware you're using or some of the more technical details of the models you're using. Throughout this tutorial, you will see some complex machine learning tasks executed in just a few lines of code by calling Keras functions.\n",
    "</div>\n",
    "    \n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2d49be",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #ffffcc; padding: 10px;\">\n",
    "    \n",
    "<h1> Requirements </h1>\n",
    "\n",
    "These notebooks should run \n",
    "\n",
    "<h2> Python Packages: </h2>\n",
    "\n",
    "* Python 3\n",
    "* Keras\n",
    "* tensorflow\n",
    "* pydot\n",
    "* graphviz\n",
    "* ipdb\n",
    "* matplotlib=3.0 \n",
    "* basemap-data-hires\n",
    "* geopy\n",
    "\n",
    "<h2> Data Requirements</h2>\n",
    "This notebook referes to some data included in the git hub repositroy\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc521a69",
   "metadata": {},
   "source": [
    "\n",
    "**Contents:**\n",
    "\n",
    "1. [Load in real and Syntheric Data](#Load-In-Provided-Data)\n",
    "2. [Augment Real Data](#Augment-Real-Data)\n",
    "3. [Merge and rescale synthetic data](#Merge_and_Rescale)\n",
    "4. [Compute bottleneck features.](#Compute_Bottleneck_features)\n",
    "5. [Train fully connected network](#Train_CNN)\n",
    "6. [Fine tune the fully connected network and the 5th convolutional block.](#Fine_Tunning) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2835391",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #cce5ff; padding: 10px;\">\n",
    "Load in all required modules (includig some auxillary code) and turn off warnings. Make sure Keras session is clear\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59f1b31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For readability: disable warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b2c4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "# general file system utilites\n",
    "import sys\n",
    "import glob\n",
    "import os\n",
    "from pathlib import Path\n",
    "# Maths and \n",
    "import numpy as np\n",
    "import numpy.ma as ma \n",
    "# Premade data is provided as pickles\n",
    "import pickle\n",
    "# Plotting utilies\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil\n",
    "# Machine learning Library Keras\n",
    "from tensorflow import keras \n",
    "#from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import losses, optimizers\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "# import axillary plotting functions\n",
    "# these functions \n",
    "from aux_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c857c4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear Keras session\n",
    "K.clear_session()  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f6eb0c",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "source": [
    "\n",
    "\n",
    "# Load In Provided Data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf70fdf2",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "source": [
    "<div style=\"background-color: #ccffcc; padding: 10px;\">\n",
    "\n",
    "    \n",
    "The model will be trained on a large dataset of synthetic interferograms which feature labels of both the type and location of any deformation. The performance improved by including a small amount of augmented real Sentinel-1 data.\n",
    "\n",
    "The first steps to prepare the data to have to be pre-made synthetic interferograms are provided in the `data` folder.\n",
    "\n",
    "<h1> Sythetic Interferograms </h1>\n",
    "\n",
    "As this is a tutorial focusing on Machine learning the Synthetic Interferograms are provided as pickle files. These files were genrated using [SyInteferoPy](https://github.com/matthew-gaddes/SyInterferoPy). [SyInteferoPy](https://github.com/matthew-gaddes/SyInterferoPy) generates synthetic images similar to those produced by Sentinel-1 satellites from the SRTM3 digital elevation model (DEM) <sup>[2]</sup>\n",
    "   \n",
    "[2] [Gaddes & Bagnardi 2019](https://agupubs.onlinelibrary.wiley.com/doi/abs/10.1029/2019JB017519)\n",
    "  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf960b5",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #ffcdcc; padding: 10px;\">    \n",
    "\n",
    "\n",
    "If you wanted to generate your own synthetic data you would need to use the tools in [SyInteferoPy](https://github.com/matthew-gaddes/SyInterferoPy). [SyInteferoPy](https://github.com/matthew-gaddes/SyInterferoPy) and the pickled volcano dem data `data/volcano_dems.pkl`\n",
    "\n",
    "\n",
    "    \n",
    "```bash\n",
    " git submodule add https://github.com/matthew-gaddes/SyInterferoPy SyInterferoPy\n",
    " dependency_paths = {'syinterferopy_bin' : 'SyInterferoPy/lib/}\n",
    " sys.path.append(dependency_paths['syinterferopy_bin'])\n",
    " \n",
    "```    \n",
    "```python\n",
    "from random_generation_functions import create_random_synthetic_ifgs  \n",
    "os.mkdir(Path(f\"./data/synthetic_data/\"))\n",
    "for file_n in range(synthetic_ifgs_n_files):\n",
    "    print(f\"Generating file {file_n} of {synthetic_ifgs_n_files} files.  \")\n",
    "    X_all, Y_class, Y_loc, Y_source_kwargs = create_random_synthetic_ifgs(volcano_dems, \n",
    "                                                                **synthetic_ifgs_settings)\n",
    "    # convert to one hot encoding (from class labels)\n",
    "    Y_class = keras.utils.to_categorical(Y_class, len(synthetic_ifgs_settings['defo_sources']), \n",
    "                                                        dtype='float32')          \n",
    "    with open(Path(f'./data/synthetic_data/data_file_{file_n}.pkl'), 'wb') as f:\n",
    "        pickle.dump(X_all[synthetic_ifgs_settings['outputs'][0]], f)                                               \n",
    "        pickle.dump(Y_class, f)\n",
    "        pickle.dump(Y_loc, f)\n",
    "    f.close()\n",
    "    del X_all, Y_class, Y_loc\n",
    "# output the settings as a text file so that we know how data were generated in the future.  \n",
    "with open(f\"./data/synthetic_data/synth_data_settings.txt\", 'w') as f:       \n",
    "    print(f\"Number of data per file : {ifg_settings['n_per_file']}\" ,file = f)\n",
    "    print(f\"Number of files: {synthetic_ifgs_n_files}\" ,file = f)\n",
    "    for key in synthetic_ifgs_settings:\n",
    "        print(f\"{key} : {synthetic_ifgs_settings[key]}\", file = f)\n",
    "```\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212cd53f",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #ffffcc; padding: 10px;\">\n",
    "    <h3>Settings for generating interfeorgrams.</h3>\n",
    "\n",
    "Passing this information to [SyInteferoPy](https://github.com/matthew-gaddes/SyInterferoPy). [SyInteferoPy](https://github.com/matthew-gaddes/SyInterferoPy) will generate 650 synthetic Interfograms\n",
    "\n",
    "* **n_per_file:** number of ifgs per data file.  \n",
    "* **synthetic_ifgs_n_files:** numer of files of synthetic data\n",
    "* **defo_sources:** deformation patterns that will be included in the dataset.  \n",
    "* **n_ifgs:** the number of synthetic interferograms to generate PER FILE\n",
    "* **n_pix:** number of 3 arc second pixels (~90m) in x and y direction\n",
    "* **outputs:**  channel outputs.  uuu = unwrapped across all 3\n",
    "* **intermediate_figure:**  if True, a figure showing the steps taken during creation of each ifg is displayed.  \n",
    "* **cov_coh_scale:** The length scale of the incoherent areas, in meters.  A smaller value creates smaller patches, and a larger one creates larger pathces.  \n",
    "* **coh_threshold:** if 1, there are no areas of incoherence, if 0 all of ifg is incoherent.  \n",
    "* **min_deformation:** deformation pattern must have a signals of at least this many metres.  \n",
    "* **max_deformation:** deformation pattern must have a signal no bigger than this many metres.  \n",
    "* **snr_threshold signal:** to noise ratio (deformation vs turbulent and topo APS) to ensure that deformation is visible.  A lower value creates more subtle deformation signals.\n",
    "* **turb_aps_mean:** turbulent APS will have, on average, a maximum strenghto this in metres (e.g 0.02 = 2cm)\n",
    "* **turb_aps_length:** turbulent APS will be correlated on this length scale, in metres.  \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d523fc1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some settings (outlined above)\n",
    "ifg_settings            = {'n_per_file'         : 50}    # number of ifgs per data file.  \n",
    "synthetic_ifgs_n_files  =  13                            # numer of files of synthetic data\n",
    "synthetic_ifgs_settings = {'defo_sources'       : ['dyke', 'sill', 'no_def'],  \n",
    "                           'n_ifgs'             : ifg_settings['n_per_file'],  \n",
    "                           'n_pix'              : 224,   \n",
    "                           'outputs'            : ['uuu'],\n",
    "                           'cov_coh_scale'      : 5000,  \n",
    "                           'coh_threshold'      : 0.7,  \n",
    "                           'min_deformation'    : 0.05,  \n",
    "                           'max_deformation'    : 0.25,\n",
    "                           'snr_threshold'      : 2.0, \n",
    "                           'turb_aps_mean'      : 0.02, \n",
    "                           'turb_aps_length'    : 5000} \n",
    "n_synth_data = ifg_settings['n_per_file'] * synthetic_ifgs_n_files\n",
    "print('\\nDetermining if files containing the synthetic deformation patterns exist...\\n ', end = '')\n",
    "synthetic_data_files = glob.glob(str(Path(f\"./data/synthetic_data/*.pkl\")))                 \n",
    "if len(synthetic_data_files) == synthetic_ifgs_n_files:\n",
    "    print(f\"\\nThe correct number of files were found ({synthetic_ifgs_n_files}) so no new ones will be generated.  \"\n",
    "          f\"\\nHowever, this doesn't guarantee that the files were made using the settings in synthetic_ifgs_settings.\" \n",
    "          f\"\\nCheck synth_data_settings.txt to be sure.   \")\n",
    "else:\n",
    "    print(f\"\\nCheck for pickles- do you have the data files required (in GitHub Repo)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e22512",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #ccffcc; padding: 10px;\">\n",
    "\n",
    "<h1> Load in Real Data</h1>\n",
    "    \n",
    "Included in this repository is a git submodule [VolcNet](https://github.com/matthew-gaddes/VolcNet) which is a set of 250 labelled unwrapped interferograms that contain labels of both the type of deformation (including examples of no deformation) and the location of deformation within the interferograms. In the form of pickle files, i.e. interferograms have been stored as masked NumPy arrays and labelled with location and deformation source.\n",
    "    \n",
    "If you have not already then in your repositoy directory please run the following code. \n",
    "    \n",
    "```bash\n",
    "git submodule init\n",
    "git submodule update --init --recursive\n",
    "```\n",
    "\n",
    "This labled data will be used to train our model.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a72fa7b",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #cce5ff; padding: 10px;\">\n",
    "The below code checks for the Volcnet files and uses plotting functions in the provided `aux_functions.py` to show the data.\n",
    "    \n",
    "For an example file it will show a plot of the interferrogram and give the lable of the deformation source (if applicable).\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1db55e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Load the real data\n",
    "# Note that these are in metres, and use one hot encoding for the class, \n",
    "# and are masked arrays (incoherence and water are masked)\n",
    "VolcNet_path = Path('./VolcNet')\n",
    "# factor to auument by.  E.g. if set to 3 and there are 250 data, there will be 650 augmented   \n",
    "real_ifg_settings       = {'augmentation_factor' : 3}\n",
    "#  get a list of the paths to all the VolcNet files\n",
    "VolcNet_files = sorted(glob.glob(str(VolcNet_path / '*.pkl')))         \n",
    "if len(VolcNet_files) == 0:\n",
    "    raise Exception('No VolcNet files have been found.'  +\n",
    "                    'Perhaps the path is wrong? Or perhaps you only want to use synthetic data?'+  \n",
    "                    'In which case, this section can be removed.  Exiting...')\n",
    "\n",
    "X_1s = []\n",
    "Y_class_1s = []\n",
    "Y_loc_1s = []\n",
    "for VolcNet_file in VolcNet_files:\n",
    "    X_1, Y_class_1, Y_loc_1 = open_VolcNet_file(VolcNet_file, synthetic_ifgs_settings['defo_sources'])\n",
    "    X_1s.append(X_1)\n",
    "    Y_class_1s.append(Y_class_1)\n",
    "    Y_loc_1s.append(Y_loc_1)\n",
    "X = ma.concatenate(X_1s, axis = 0)\n",
    "Y_class = np.concatenate(Y_class_1s, axis = 0)\n",
    "Y_loc = np.concatenate(Y_loc_1s, axis = 0)\n",
    "del X_1s, Y_class_1s, Y_loc_1s, X_1, Y_class_1, Y_loc_1\n",
    "# plot the data in it (note that this can be across multiople windows) \n",
    "plot_data_class_loc_caller(X[:30,], Y_class[:30,], Y_loc[:30,], source_names = ['dyke', 'sill', 'no def'], window_title = 'Sample of Real data')               \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78557fb9",
   "metadata": {},
   "source": [
    "# Augment Real Data\n",
    "\n",
    "<div style=\"background-color: #ccffcc; padding: 10px;\">\n",
    "    \n",
    "To improve the performance of the model real data is incorporated. Because we can't include as much real data as the synthetic data we must 'augment' the data into the same size as the number of synthetic interferograms by creating random flips,  rotations,  and translations\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6095dd17",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #cce5ff; padding: 10px;\">\n",
    "As this augmentation is purely data manipulation we'll use some functions from `aux_functions.py` to help augment the set of 250 interferograms and generate a set of 650 augmented interferograms. The below code will generate pickle files of the augmented data that you can reuse so this step only needs to be done once.\n",
    "    \n",
    "<br>    \n",
    "    \n",
    "If you've already done this the below code will check if the pickles already exist and only calculated the augmented data if required\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c1174c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_augmented_files = int((X.shape[0] * real_ifg_settings['augmentation_factor']) / ifg_settings['n_per_file'])                   # detemine how many files will be needed, given the agumentation factor.  \n",
    "print('    Determining if files containing the augmented real data exist.')\n",
    "real_augmented_files = glob.glob(str(Path(f\"./data/real/augmented/*.pkl\")))             #\n",
    "if len(real_augmented_files) == n_augmented_files:\n",
    "    print(f\"    The correct number of augmented real data files were found ({n_augmented_files}) \"\n",
    "          f\"so no new ones will be generated.  \"\n",
    "          f\"However, this doesn't guarantee that the files were made using the current real data.  \")\n",
    "else:\n",
    "        try:\n",
    "            shutil.rmtree(str(Path(f\"./data/real/augmented/\")))\n",
    "        except:\n",
    "            pass\n",
    "        os.mkdir((Path(f\"./data/real/\")))\n",
    "        os.mkdir((Path(f\"./data/real/augmented/\")))\n",
    "        print(f\"There are {X.shape[0]} real data and the augmentation factor is set\" +\n",
    "              f\"to {real_ifg_settings['augmentation_factor']}.  \")\n",
    "        print(f\"    With {ifg_settings['n_per_file']} data per file, the nearest integer\" + \n",
    "              f\"number of files is {n_augmented_files}.  \")\n",
    "        # loop through each file that is to be made\n",
    "        for n_augmented_file in range(n_augmented_files):                                                                               \n",
    "            print(f'    File {n_augmented_file} of {n_augmented_files}...', end = '')  \n",
    "            X_sample, Y_class_sample, Y_loc_sample = choose_for_augmentation(X, Y_class, Y_loc,                                         # make a new selection of the data with balanced classes\n",
    "                                                                              n_per_class = int(X.shape[0] / Y_class.shape[1]))          # set it so there are as many per class as there are (on average) for the real data.  \n",
    "            X_aug, Y_class_aug, Y_loc_aug = augment_data(X_sample, Y_class_sample, Y_loc_sample,                                        # augment the sample of real data\n",
    "                                                          n_data = ifg_settings['n_per_file'])                                           # make as many new data as are set to be in a single file.  \n",
    "        \n",
    "            with open(f\"./data/real/augmented/data_file_{n_augmented_file}.pkl\", 'wb') as f:                                        # save the output as a pickle\n",
    "                pickle.dump(X_aug, f)\n",
    "                pickle.dump(Y_class_aug, f)\n",
    "                pickle.dump(Y_loc_aug, f)\n",
    "            f.close()\n",
    "            print('Done!')\n",
    "        # fill variable with new generated files\n",
    "        real_augmented_files = glob.glob(str(Path(f\"./data/real/augmented/*.pkl\")))  \n",
    "        print('Done!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6cde62",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #ccffcc; padding: 10px;\">\n",
    "<h1> Plot Augmented Data </h1>\n",
    "\n",
    "Check the produced data looks sensible, this should look similar to our real data just different transformations\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42aa2770",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "open_datafile_and_plot(\"./data/real/augmented/data_file_0.pkl\", n_data = 15, window_title = '03 Sample of augmented real data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a00a955",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #ccffcc; padding: 10px;\">\n",
    "<h1> Plot Synthetic Data </h1>\n",
    "\n",
    "Check the Synthetic data looks sensible, this should also look similar to our real data!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d23e54",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "open_datafile_and_plot(f\"data/synthetic_data/data_file_0.pkl\", n_data = 15, \n",
    "                       window_title ='01 Sample of synthetic data')  # open and plot the data in 1 file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f50e9f",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #ccffcc; padding: 10px;\">\n",
    "\n",
    "<h1> Merging real and synthetic inteferograms and rescaling to CCN's input range</h1>\n",
    "\n",
    "First, we're going to merge our two datasets and format them into an output range suitable for the CNN used. E.g. our data might be in meters and rads and we need to rescale to values in the RGB range 0-255 (python's first indice is 0 so 0 -255 gives 256 values)\n",
    "\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9220ff2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_settings = {'input_range': {'min':0, 'max':255}}     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275cc156",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "def merge_and_rescale_data(synthetic_data_files, real_data_files, output_range = {'min':0, 'max':225}):\n",
    "    \"\"\" Given a list of synthetic data files and real data files (usually the augmented real data),\n",
    "    \n",
    "    Inputs:\n",
    "        synthetic_data_files | list of Paths or string | locations of the .pkl files containing the masked arrays\n",
    "        reak_data_files      | list of Paths or string | locations of the .pkl files containing the masked arrays\n",
    "        output_range         | dict                    | min and maximum of each channel in each image. \n",
    "                                                         Should be set to suit the CNN being used.  \n",
    "    Returns:\n",
    "        .npz files in step_04_merged_rescaled_data\n",
    "    History:\n",
    "        2020_10_29 | MEG | Written\n",
    "        2021_01_06 | MEG | Fix bug in that mixed but not rescaled data was being written to the numpy arrays.  \n",
    "    \"\"\"  \n",
    "    def data_channel_checker(X, n_cols = None, window_title = None):\n",
    "        \"\"\" Plot some of the data in X.   All three channels are shown.  \n",
    "        \"\"\"      \n",
    "        if n_cols == None:       # if n_cols is None, we'll plot all the data\n",
    "            n_cols = X.shape[0]   # so n_cols is the number of data\n",
    "            plot_args = np.arange(0, n_cols) # and we'll be plotting each of them\n",
    "        else:\n",
    "            plot_args = np.random.randint(0, X.shape[0], n_cols)        # else, pick some at random to plot\n",
    "        f, axes = plt.subplots(3,n_cols)\n",
    "        if window_title is not None:\n",
    "            f.canvas.set_window_title(window_title)\n",
    "        for plot_n, im_n in enumerate(plot_args):                           # loop through each data (column)               \n",
    "            axes[0, plot_n].set_title(f\"Data: {im_n}\")\n",
    "            for channel_n in range(3):                                      # loop through each row\n",
    "                axes[channel_n, plot_n].imshow(X[im_n, :,:,channel_n])\n",
    "                if plot_n == 0:\n",
    "                    axes[channel_n, plot_n].set_ylabel(f\"Channel {channel_n}\")\n",
    "\n",
    "    if len(synthetic_data_files) != len(real_data_files):\n",
    "        raise Exception('This funtion is only designed to be used when the number of real and synthetic data files are the same.  Exiting.  ')\n",
    "\n",
    "    n_files = len(synthetic_data_files)        \n",
    "    out_file = 0\n",
    "    try:\n",
    "            shutil.rmtree(str(Path(f\"./data/merged_out/\")))\n",
    "    except:\n",
    "        pass\n",
    "    os.mkdir((Path(f\"./data/merged_out\")))# \n",
    "    for n_file in range(n_files):\n",
    "        print(f'    Opening and merging file {n_file} of each type... ', end = '')\n",
    "        with open(real_data_files[n_file], 'rb') as f:       # open the real data file\n",
    "            X_real = pickle.load(f)\n",
    "            Y_class_real = pickle.load(f)\n",
    "            Y_loc_real = pickle.load(f)\n",
    "        f.close()    \n",
    "        \n",
    "        with open(synthetic_data_files[n_file], 'rb') as f:       # open the synthetic data file\n",
    "            X_synth = pickle.load(f)\n",
    "            Y_class_synth = pickle.load(f)\n",
    "            Y_loc_synth = pickle.load(f)\n",
    "        f.close()    \n",
    "\n",
    "        X = ma.concatenate((X_real, X_synth), axis = 0)        # concatenate the data\n",
    "        Y_class = ma.concatenate((Y_class_real, Y_class_synth), axis = 0)    # and the class labels\n",
    "        Y_loc = ma.concatenate((Y_loc_real, Y_loc_synth), axis = 0)        # and the location labels\n",
    "        \n",
    "        mix_index = np.arange(0, X.shape[0])          # mix them, get a lis of arguments for each data \n",
    "        np.random.shuffle(mix_index)            # shuffle the arguments\n",
    "        X = X[mix_index,]               # reorder the data using the shuffled arguments\n",
    "        Y_class = Y_class[mix_index]     # reorder the class labels\n",
    "        Y_loc = Y_loc[mix_index]      # and the location labels\n",
    "        # resacle the data from metres/rads etc. to desired input range of cnn (e.g. [0, 255]), \n",
    "        # and convert to numpy array\n",
    "        X_rescale = custom_range_for_CNN(X, output_range, mean_centre = False)                  \n",
    "        data_mid = int(X_rescale.shape[0] / 2)\n",
    "        np.savez(f'data/merged_out/data_file_{out_file}.npz', \n",
    "                 X = X_rescale[:data_mid,:,:,:], \n",
    "                 Y_class= Y_class[:data_mid,:], \n",
    "                 Y_loc = Y_loc[:data_mid,:])           # save the first half of the data\n",
    "        out_file += 1                                                                                                                                                   # after saving once, update\n",
    "        np.savez(f'data/merged_out/data_file_{out_file}.npz', \n",
    "                 X = X_rescale[data_mid:,:,:,:], \n",
    "                 Y_class= Y_class[data_mid:,:],\n",
    "                 Y_loc = Y_loc[data_mid:,:])           # save the second half of the data\n",
    "        out_file += 1                                                                                                                                                   # and after saving again, update again.  \n",
    "        print('Done.  ')\n",
    "        \n",
    "def expand_to_r4(r2_array, shape = (224,224)):\n",
    "    \"\"\"\n",
    "    Calcaulte something for every image and channel in rank 4 data (e.g. 100x224x224x3 to get 100x3)\n",
    "    Expand new rank 2 to size of original rank 4 for elemtiwise operations\n",
    "    \"\"\"\n",
    "    \n",
    "    r4_array = r2_array[:, np.newaxis, np.newaxis, :]\n",
    "    r4_array = np.repeat(r4_array, shape[0], axis = 1)\n",
    "    r4_array = np.repeat(r4_array, shape[1], axis = 2)\n",
    "    return r4_array\n",
    "\n",
    "def custom_range_for_CNN(r4_array, min_max, mean_centre = False):\n",
    "    \"\"\" Rescale a rank 4 array so that each channel's image lies in custom range\n",
    "    e.g. input with range of (-5, 15) is rescaled to (-125 125) or (-1 1) for use with VGG16.  \n",
    "    Designed for use with masked arrays.  \n",
    "    Inputs:\n",
    "        r4_array | r4 masked array | works with masked arrays?  \n",
    "        min_max | dict | 'min' and 'max' of range desired as a dictionary.  \n",
    "        mean_centre | boolean | if True, each image's channels are mean centered.  \n",
    "    Returns:\n",
    "        r4_array | rank 4 numpy array | masked items are set to zero, rescaled so that each channel for each image lies between min_max limits.  \n",
    "    History:\n",
    "        2019/03/20 | now includes mean centering so doesn't stretch data to custom range.  \n",
    "                    Instead only stretches until either min or max touches, whilst mean is kept at 0\n",
    "        2020/11/02 | MEG | Update so range can have a min and max, and not just a range\n",
    "        2021/01/06 | MEG | Upate to work with masked arrays.  Not test with normal arrays.\n",
    "    \"\"\"\n",
    "    if mean_centre:\n",
    "        # get the average for each image (in all thre channels)\n",
    "        im_channel_means = ma.mean(r4_array, axis = (1,2)) \n",
    "        # expand to r4 so we can do elementwise manipulation\n",
    "        im_channel_means = expand_to_r4(im_channel_means, r4_array[0,:,:,0].shape)          \n",
    "        # do mean centering    \n",
    "        r4_array -= im_channel_means                                                                        \n",
    "\n",
    "    # get the minimum of each image and each of its channels    \n",
    "    im_channel_min = ma.min(r4_array, axis = (1,2))\n",
    "    # exapnd to rank 4 for elementwise applications\n",
    "    im_channel_min = expand_to_r4(im_channel_min, r4_array[0,:,:,0].shape)\n",
    "    # set so lowest channel for each image is 0\n",
    "    r4_array -= im_channel_min                                                              \n",
    "    # get the maximum of each image and each of its channels\n",
    "    im_channel_max = ma.max(r4_array, axis = (1,2)) \n",
    "    # make suitable for elementwise applications\n",
    "    im_channel_max = expand_to_r4(im_channel_max, r4_array[0,:,:,0].shape) \n",
    "    r4_array /= im_channel_max  # should now be in range [0, 1]\n",
    "    \n",
    "    r4_array *= (min_max['max'] - min_max['min'])    # should now be in range [0, new max-min]\n",
    "    r4_array += min_max['min']                # and now in range [new min, new max]\n",
    "    # convert to numpy array, maksed incoherent areas are set to zero.  \n",
    "    r4_nparray = r4_array.filled(fill_value = 0)         \n",
    "    return r4_nparray \n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55451c22",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #ccffcc; padding: 10px;\">\n",
    "\n",
    "<h1> Plot the new reformatted data</h1>\n",
    "\n",
    "The following cell calls the `merge_and_rescale` function and plots the output. You will notice these images still look similar to our original images just on a new scale. \n",
    "\n",
    "This might take a couple minutes.... \n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7caeacfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the paths to each file of real data\n",
    "merge_and_rescale_data(synthetic_data_files, real_augmented_files, cnn_settings['input_range'])                                   # merge the real and synthetic data, and rescale it into the correct range for use with the CNN\n",
    "open_datafile_and_plot(\"./data/merged_out/data_file_0.npz\", n_data = 15,\n",
    "                       window_title = ' 04 Sample of merged and rescaled data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7834e231",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "\n",
    "<div style=\"background-color: #e6ccff; padding: 10px;\">\n",
    "    \n",
    "<h1> Bottleneck Features </h1>\n",
    "    \n",
    "To train the model using different types of synthetic data, [bottleneck learning](https://towardsdatascience.com/a-comprehensive-hands-on-guide-to-transfer-learning-with-real-world-applications-in-deep-learning-212bf3b2f27a) is used. First, we compute the results from passing our entire dataset through the first five blocks of VGG16, before then training only the fully connected parts of our network (i.e.the classification output). \n",
    "    \n",
    "    \n",
    "<a href=\"https://eartharxiv.org/repository/view/1969/\">\n",
    "<img src=\"https://raw.githubusercontent.com/cemac/LIFD_ENV_ML_NOTEBOOKS/main/ConvolutionalNeuralNetworks/images/bottleneckgaddes2021.png\" width=\"500\" height=\"600\" >\n",
    "</a>\n",
    "\n",
    "</div>    \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b113cd23",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #cce5ff; padding: 10px;\">\n",
    "\n",
    "this uses the Keras VGG16 module which if the machine you are using has GPU's will automatically use. The below code \n",
    "\n",
    "```python\n",
    "    vgg16_block_1to5 = VGG16(weights='imagenet', include_top=False, input_shape = (224,224,3))\n",
    "```\n",
    "    \n",
    "loads the first 5 convolutional blocks of VVG16 model trained for [imagenet](https://image-net.org/challenges/LSVRC/) and tells it our input interferograms will be in the shape 224 X 224 x 3\n",
    "    \n",
    " ```python\n",
    "    X_btln = vgg16_block_1to5.predict(X, verbose = 1)\n",
    "  ```\n",
    " \n",
    "will pass the data through the blocks to create a tensor of shape (7 x 7 x 512) \n",
    "    \n",
    "<hr>\n",
    "    \n",
    "If the machine you are using does not have GPUS then \n",
    "\n",
    "    \n",
    "The below segment of code may take a while if running on your laptop, if this is going to take too long then set\n",
    "    \n",
    "```python\n",
    "UsePreMadeBottlenecks = True\n",
    "\n",
    "```\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7eabcdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "UsePreMadeBottlenecks = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6a1196",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Compute bottleneck features):\n",
    "\n",
    "# load the first 5 (convolutional) blocks of VGG16 and their weights.\n",
    "vgg16_block_1to5 = VGG16(weights='imagenet', include_top=False, input_shape = (224,224,3))     \n",
    "\n",
    "  \n",
    "data_out_files = sorted(glob.glob(f'./data/merged_out/*.npz'))       \n",
    "\n",
    "if UsePreMadeBottlenecks is False:\n",
    "    try:\n",
    "            shutil.rmtree(str(Path(f\"./data/bottleneck_out/\")))\n",
    "    except:\n",
    "        pass\n",
    "    os.mkdir((Path(f\"./data/bottleneck_out\")))# \n",
    "    bottleneck_folder = 'bottleneck_out'\n",
    "    # get a list of the files output by step 05 (augmented real data and synthetic data mixed and \n",
    "    # rescaed to correct range, with 0s for masked areas.  )\n",
    "    for file_n, data_out_file in enumerate(data_out_files):   \n",
    "        # loop through each of the step 05 files.  \n",
    "        print(f'Bottlneck file {file_n}:') \n",
    "        data_out_file = Path(data_out_file)                                                                                     # convert to path \n",
    "        bottleneck_file_name = data_out_file.parts[-1].split('.')[0]                                                            # and get last part which is filename    \n",
    "        data = np.load(data_out_file)                                                                                           # load the numpy file\n",
    "        X = data['X']                                                                                                           # extract the data for it\n",
    "        Y_class = data['Y_class']                                                                                               # and class labels.  \n",
    "        Y_loc = data['Y_loc']                                                                                                   # and location labels.  \n",
    "        X_btln = vgg16_block_1to5.predict(X, verbose = 1)      \n",
    "        # predict up to bottleneck    \n",
    "        np.savez(f'data/{bottleneck_folder}/{bottleneck_file_name}_bottleneck.npz', \n",
    "                 X = X_btln, Y_class = Y_class, Y_loc = Y_loc)   \n",
    "        # save the bottleneck file, and the two types of label.\n",
    "else:\n",
    "    print('using premade bottleneck files')\n",
    "    bottleneck_folder = 'bottleneck_provided'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f60a69c",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e6ccff; padding: 10px;\">\n",
    "    \n",
    "<h1> Training the Neural Network </h1>\n",
    "\n",
    "</div>    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014619c0",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #cce5ff; padding: 10px;\">\n",
    "\n",
    " First, we need two functions to divide the list into a training and testing dataset. Two functions are written below to divide the data files and bottleneck files into testing and training datasets and to load various features into arrays in your computers RAM\n",
    "\n",
    "```python\n",
    "    \n",
    "\n",
    "train_test_validate = file_list_divider(data_files, cnn_settings['n_files_train'], \n",
    "                                         cnn_settings['n_files_validate'], \n",
    "                                         cnn_settings['n_files_test']) \n",
    "# assign the outputs    \n",
    "[data_files_train, data_files_validate, data_files_test] = train_test_validate \n",
    "```\n",
    "    \n",
    "creates a set of files to use for training, validation and testing based of perscibed CNN settings\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51058f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def file_list_divider(file_list, n_files_train, n_files_validate, n_files_test):\n",
    "    \"\"\" Given a list of files, divide it up into training, validating, and testing lists.  \n",
    "    Inputs\n",
    "        file_list | list | list of files\n",
    "        n_files_train | int | Number of files to be used for training\n",
    "        n_files_validate | int | Number of files to be used for validation (during training)\n",
    "        n_files_test | int | Number of files to be used for testing\n",
    "    Returns:\n",
    "        file_list_train | list | list of training files\n",
    "        file_list_validate | list | list of validation files\n",
    "        file_list_test | list | list of testing files\n",
    "    History:\n",
    "        2019/??/?? | MEG | Written\n",
    "        2020/11/02 | MEG | Write docs\n",
    "        \"\"\"\n",
    "    file_list_train = file_list[:n_files_train]\n",
    "    file_list_validate = file_list[n_files_train:(n_files_train+n_files_validate)]\n",
    "    file_list_test = file_list[(n_files_train+n_files_validate) : (n_files_train+n_files_validate+n_files_test)]\n",
    "    return file_list_train, file_list_validate, file_list_test\n",
    "\n",
    "def file_merger(files): \n",
    "    \"\"\"Given a list of files, open them and merge into one array.  \n",
    "    Inputs:\n",
    "        files | list | list of paths to the .npz files\n",
    "    Returns\n",
    "        X | r4 array | data\n",
    "        Y_class | r2 array | class labels, ? x n_classes\n",
    "        Y_loc | r2 array | locations of signals, ? x 4 (as x,y, width, heigh)\n",
    "    History:\n",
    "        2020/10/?? | MEG | Written\n",
    "        2020/11/11 | MEG | Update to remove various input arguments\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def open_synthetic_data_npz(name_with_path):\n",
    "        \"\"\"Open a file data file \"\"\"  \n",
    "        data = np.load(name_with_path)\n",
    "        X = data['X']\n",
    "        Y_class = data['Y_class']\n",
    "        Y_loc = data['Y_loc']\n",
    "        return X, Y_class, Y_loc\n",
    "\n",
    "    n_files = len(files)\n",
    "    for i, file in enumerate(files):\n",
    "        X_batch, Y_class_batch, Y_loc_batch = open_synthetic_data_npz(file)\n",
    "        if i == 0:\n",
    "            n_data_per_file = X_batch.shape[0]\n",
    "            # initate array, rank4 for image, get the size from the first file\n",
    "            X = np.zeros((n_data_per_file * n_files, X_batch.shape[1], X_batch.shape[2], X_batch.shape[3]))\n",
    "            # should be flexible with class labels or one hot encoding\n",
    "            Y_class = np.zeros((n_data_per_file  * n_files, Y_class_batch.shape[1]))              \n",
    "            Y_loc = np.zeros((n_data_per_file * n_files, 4))    # four columns for bounding box\n",
    "            \n",
    "        \n",
    "        X[i*n_data_per_file:(i*n_data_per_file)+n_data_per_file,:,:,:] = X_batch\n",
    "        Y_class[i*n_data_per_file:(i*n_data_per_file)+n_data_per_file,:] = Y_class_batch\n",
    "        Y_loc[i*n_data_per_file:(i*n_data_per_file)+n_data_per_file,:] = Y_loc_batch\n",
    "    \n",
    "    return X, Y_class, Y_loc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70293e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tain the fully connected part of the network)\n",
    "cnn_settings = {'input_range'       : {'min':0, 'max':255}}\n",
    "# the number of files that will be used to train the network\n",
    "cnn_settings['n_files_train']     = 22      \n",
    "# the number of files that wil be used to validate the network (i.e. passed through once per epoch)\n",
    "cnn_settings['n_files_validate']  = 2       \n",
    "# the number of files held back for testing. \n",
    "cnn_settings['n_files_test']      = 2       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b1af53",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_files = sorted(glob.glob(f'./data/merged_out/*npz'), key = os.path.getmtime)    # make list of data files\n",
    "# and make a list of bottleneck files (ie files that have been passed through the first 5 blocks of vgg16)\n",
    "bottleneck_files = sorted(glob.glob('./data/'+str(bottleneck_folder)+'/'+f'*npz'), key = os.path.getmtime) \n",
    "if len(data_files) < (cnn_settings['n_files_train'] + cnn_settings['n_files_validate'] + cnn_settings['n_files_test']):\n",
    "    raise Exception(f\"There are {len(data_files)} data files, but {cnn_settings['n_files_train']} have been selected for training, \"\n",
    "                    f\"{cnn_settings['n_files_validate']} for validation, and {cnn_settings['n_files_test']} for testing, \"\n",
    "                    f\"which sums to greater than the number of data files.  Perhaps adjust the number of files used for the training stages? \"\n",
    "                    f\"For now, exiting.\")\n",
    "\n",
    "data_files_train, data_files_validate, data_files_test = file_list_divider(data_files, \n",
    "                                                                           cnn_settings['n_files_train'], \n",
    "                                                                           cnn_settings['n_files_validate'], \n",
    "                                                                           cnn_settings['n_files_test'])                              # divide the files into train, validate and test\n",
    "# also divide the bottleneck files\n",
    "bottleneck_files_train, bottleneck_files_validate, bottleneck_files_test = file_list_divider(bottleneck_files, \n",
    "                                                                            cnn_settings['n_files_train'],\n",
    "                                                                            cnn_settings['n_files_validate'], \n",
    "                                                                            cnn_settings['n_files_test'])      \n",
    "\n",
    "\n",
    "# Open all the validation data to RAM\n",
    "X_validate, Y_class_validate, Y_loc_validate      = file_merger(data_files_validate)                       \n",
    "# Open the validation data bottleneck features to RAM\n",
    "X_validate_btln, Y_class_validate, Y_loc_validate = file_merger(bottleneck_files_validate)     \n",
    "# Open the test data to RAM\n",
    "X_test, Y_class_test, Y_loc_test                  = file_merger(data_files_test)     \n",
    "# Open the test data bottleneck features to RAM\n",
    "X_test_btln, Y_class_test_btln, Y_loc_test_btln   = file_merger(bottleneck_files_test)  \n",
    "\n",
    "\n",
    "print(f\"    There are {len(data_files)} data files.  {len(data_files_train)} will be used for training,\"    \n",
    "      f\"{len(data_files_validate)} for validation, and {len(data_files_test)} for testing.  \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00399d0f",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e6ccff; padding: 10px;\">\n",
    "    \n",
    "<h1> Define two headed model and training </h1>\n",
    "\n",
    "The interferograms of shape (2242243) are passed through the five convolutional blocks of VGG16 to create a tensor of shape (77512).  This is flattened to make a vector of size 25,088, before being passed through fully connected layers of size 256, 128, and an output layer of size three (i.e.,  dyke,  sill/point,  or no deformation). This is done in our `define_two_head_model` which takes the input from our VGG16 model to give our 3 class output.\n",
    "    \n",
    "```python\n",
    "vgg16_block_1to5 = VGG16(weights='imagenet', include_top=False, input_shape = (224,224,3))       \n",
    "```\n",
    "\n",
    "we then need to make the input to the fully connected model the same shape as the output of the 5th block of vgg16 then build the full connected part of the model and get the two model outputs using the `define_two_head_model` function\n",
    "\n",
    "```python\n",
    "fc_model_input = Input(shape = vgg16_block_1to5.output_shape[1:])    \n",
    "output_class, output_loc = define_two_head_model(fc_model_input, \n",
    "                                                 len(synthetic_ifgs_settings['defo_sources'])) \n",
    "``` \n",
    "\n",
    "to build our headed model\n",
    "\n",
    "```python    \n",
    "vgg16_2head_fc = Model(inputs=fc_model_input, outputs=[output_class, output_loc])                        \n",
    "            \n",
    "```\n",
    "       \n",
    "</div>    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc339512",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_two_head_model(model_input, n_class_outputs = 3):\n",
    "    \"\"\" Define the two headed model that we have designed to performed classification and localisation.  \n",
    "    Inputs:\n",
    "        model_input | tensorflow.python.framework.ops.Tensor | \n",
    "                      The shape of the tensor that will be input to our model. \n",
    "                      Usually the output of VGG16 (?x7x7x512)  Nb ? = batch size.  \n",
    "        n_class_output | int | For a one hot encoding style output, there must be as many neurons as classes\n",
    "    Returns:\n",
    "        output_class |tensorflow.python.framework.ops.Tensor | \n",
    "                      The shape of the tensor output by the classifiction head.  Usually ?x3\n",
    "        output_loc | tensorflow.python.framework.ops.Tensor | \n",
    "                     The shape of the tensor output by the localisation head.  Usually ?x4\n",
    "    History:\n",
    "        2020_11_11 | MEG | Written\n",
    "    \"\"\"    \n",
    "    vgg16_block_1to5_flat = Flatten(name = 'vgg16_block_1to5_flat')(model_input)  \n",
    "    # flatten the model input (ie deep representation turned into a column vector)\n",
    "    # 1: the clasification head\n",
    "    x = Dropout(0.2, name='class_dropout1')(vgg16_block_1to5_flat)\n",
    "    # add a fully connected layer\n",
    "    x = Dense(256, activation='relu', name='class_dense1')(x)                                                 \n",
    "    x = Dropout(0.2, name='class_dropout2')(x)\n",
    "    # add a fully connected layer\n",
    "    x = Dense(128, activation='relu', name='class_dense2')(x)                                                \n",
    "    # and an ouput layer with 7 outputs (ie one per label)\n",
    "    output_class = Dense(n_class_outputs, activation='softmax',  name = 'class_dense3')(x)                 \n",
    "    \n",
    "    \n",
    "    # 2: the localization head\n",
    "    x = Dense(2048, activation='relu', name='loc_dense1')(vgg16_block_1to5_flat) \n",
    "    # add a fully connected layer\n",
    "    x = Dense(1024, activation='relu', name='loc_dense2')(x)                                                \n",
    "    # add a fully connected layer\n",
    "    x = Dense(1024, activation='relu', name='loc_dense3')(x)                                                \n",
    "    x = Dropout(0.2, name='loc_dropout1')(x)\n",
    "    # add a fully connected layer\n",
    "    x = Dense(512, activation='relu', name='loc_dense4')(x)                                                 \n",
    "    # add a fully connected layer\n",
    "    x = Dense(128, activation='relu', name='loc_dense5')(x)                                                 \n",
    "    output_loc = Dense(4, name='loc_dense6')(x)        \n",
    "    \n",
    "    return output_class, output_loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0296e7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define, compile, and train the model\n",
    "# VGG16 is used for its convolutional layers and weights (but no fully connected part as we define out own )\n",
    "vgg16_block_1to5 = VGG16(weights='imagenet', include_top=False, input_shape = (224,224,3))       \n",
    "# the input to the fully connected model must be the same shape as the output of the 5th block of vgg16\n",
    "fc_model_input = Input(shape = vgg16_block_1to5.output_shape[1:])               \n",
    "# build the full connected part of the model, and get the two model outputs\n",
    "output_class, output_loc = define_two_head_model(fc_model_input, len(synthetic_ifgs_settings['defo_sources']))  \n",
    "# define the model.  Input is the shape of vgg16 block 1 to 5 output, and there are two outputs (hence list)\n",
    "vgg16_2head_fc = Model(inputs=fc_model_input, outputs=[output_class, output_loc])                        \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd579c4",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #ccffcc; padding: 10px;\">\n",
    "\n",
    "<h1> Plot model</h1>\n",
    "\n",
    "**Note** Graphviz might not work on all systems, the code below will provide an alternative solution if graphviz fails    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa88484b",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    os.mkdir((Path(f\"./data/train_fully_connected_model\")))\n",
    "except:\n",
    "    pass\n",
    "try:\n",
    "    plot_model(vgg16_2head_fc, to_file=f'data/train_fully_connected_model/vgg16_2head_fc.png',            \n",
    "           # also plot the model.  This funtcion is known to be fragile due to Graphviz dependencies.  \n",
    "           show_shapes = True, show_layer_names = True)\n",
    "except:\n",
    "    vgg16_2head_fc.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00618efd",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e6ccff; padding: 10px;\">\n",
    "    \n",
    "<h1> Define two headed model and training </h1>\n",
    "\n",
    "Now we can compile the model passing in some optimizations: the standard [Adam gradient-based optimizer](https://arxiv.org/abs/1412.6980) and the [loss functions](https://keras.io/api/losses/) ( Cross-Entropy loss as Softmax is to be used) and request the accuracy metric to be reported \n",
    "\n",
    "```python\n",
    "vgg16_2head_fc.compile(optimizer = opt_used, loss=[loss_class, loss_loc], \n",
    "                       # compile the model\n",
    "                       loss_weights = fc_loss_weights, metrics=['accuracy'])  \n",
    "```\n",
    "\n",
    "And write a function to train out double-headed model\n",
    "\n",
    "```python\n",
    "train_double_network(vgg16_2head_fc,  bottleneck_files_train,n_epochs_fc, \n",
    "                     ['class_dense3_loss','loc_dense6_loss'], Xvalidate_btln, \n",
    "                     Y_class_validate, Y_loc_validate,\n",
    "                     len(synthetic_ifgs_settings['defo_sources']))\n",
    "```\n",
    "\n",
    "which takes our compiled model `vgg16_2head_fc` and our subset of bottleneck files  and trains over a number of \n",
    "\"epochs\" here set to 10 for speed rather than accuracy. \n",
    "    \n",
    "<hr>\n",
    "    \n",
    "Training the model may take a few minutes ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab4b8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_double_network(model, files, n_epochs, loss_names,\n",
    "                                    X_validate, Y_class_validate, Y_loc_validate, n_classes):\n",
    "    \"\"\"Train a double headed model using training data stored in separate files.  \n",
    "    Inputs:\n",
    "        model | keras model | the model to be trained\n",
    "        files | list | list of paths and filenames for the files used during training\n",
    "        n_epochs | int | number of epochs to train for\n",
    "        loss names | list | names of outputs of losses (e.g. \"class_dense3_loss)\n",
    "    Returns\n",
    "        model | keras model | updated by the fit process\n",
    "        metrics_loss | r2 array | columns are: total loss/class loss/loc loss /validate total loss/validate \n",
    "                                               class loss/ validate loc loss\n",
    "        metrics_class | r2 array | columns are class accuracy, validation class accuracy\n",
    "        \n",
    "    2019/03/25 | Written.  \n",
    "    \"\"\"  \n",
    "    n_files_train = len(files)          # get the number of training files\n",
    "    \n",
    "    metrics_class = np.zeros((n_files_train*n_epochs, 2))   # train class acuracy, valiate class accuracy\n",
    "    # total loss/class loss/loc loss /validate total loss/validate class loss/ validate loc loss\n",
    "    metrics_loss = np.zeros((n_files_train*n_epochs, 6))     \n",
    "    for e in range(n_epochs):     # loop through the number of epochs\n",
    "        for file_num, file in enumerate(files):    # for each epoch, loop through all files once\n",
    "        \n",
    "            data = np.load(file)\n",
    "            X_batch = data['X']\n",
    "            Y_batch_class = data['Y_class']\n",
    "            Y_batch_loc = data['Y_loc']\n",
    "            \n",
    "            if n_classes !=  Y_batch_class.shape[1]:\n",
    "                # convert to one hot encoding (from class labels)\n",
    "                Y_batch_class = keras.utils.to_categorical(Y_batch_class, n_classes, dtype='float32')                     \n",
    "\n",
    "            history_train_temp = model.fit(X_batch, [Y_batch_class, Y_batch_loc], batch_size=32,\n",
    "                                           epochs=1, verbose = 0)\n",
    "            # main loss   \n",
    "            metrics_loss[(e*n_files_train)+file_num, 0] = history_train_temp.history['loss'][0]   \n",
    "            # class loss\n",
    "            metrics_loss[(e*n_files_train)+file_num, 1] = history_train_temp.history[loss_names[0]][0]\n",
    "            # localization loss\n",
    "            metrics_loss[(e*n_files_train)+file_num, 2] = history_train_temp.history[loss_names[1]][0]   \n",
    "            metrics_class[(e*n_files_train)+file_num, 0] = history_train_temp.history['class_dense3_accuracy'][0]           # classification accuracy        \n",
    "            print(f'Epoch {e}, file {file_num}: Loss = {round(metrics_loss[(e*n_files_train)+file_num, 0],0)}, '\n",
    "                                       f'Class. loss = {round(metrics_loss[(e*n_files_train)+file_num, 1],2)}, '\n",
    "                                       f'Class. acc. = {round(metrics_class[(e*n_files_train)+file_num, 0],2)}, '\n",
    "                                         f'Loc. loss = {round(metrics_loss[(e*n_files_train)+file_num, 2],0)}')\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "        history_validate_temp = model.evaluate(X_validate, [Y_class_validate, Y_loc_validate], \n",
    "                                               batch_size = 32, verbose = 0)\n",
    "        metrics_loss[(e*n_files_train)+file_num, 3] = history_validate_temp[0]     # main loss\n",
    "        metrics_loss[(e*n_files_train)+file_num, 4] = history_validate_temp[1]     # class loss\n",
    "        metrics_loss[(e*n_files_train)+file_num, 5] = history_validate_temp[2]     # localisation loss\n",
    "        metrics_class[(e*n_files_train)+file_num, 1] = history_validate_temp[3]    # classification  accuracy\n",
    "        print(f'Epoch {e}, valid.: Loss = {round(metrics_loss[(e*n_files_train)+file_num, 3],0)}, '\n",
    "                          f'Class. loss = {round(metrics_loss[(e*n_files_train)+file_num, 4],2)}, '\n",
    "                          f'Class. acc. = {round(metrics_class[(e*n_files_train)+file_num, 1],2)}, '\n",
    "                            f'Loc. loss = {round(metrics_loss[(e*n_files_train)+file_num, 5],0)}')\n",
    "    \n",
    "    # class loss, validate class loss, class accuracy, validate class accuracy\n",
    "    metrics_class = np.hstack((metrics_loss[:,1:2], metrics_loss[:,4:5], metrics_class ))     \n",
    "    # localisation loss, validate localisation loss, localisation accuracy, validate localisation accuracy\n",
    "    metrics_localisation = np.hstack((metrics_loss[:,2:3], metrics_loss[:,5:]))                        \n",
    "    # class accuracy, validate class accuracy\n",
    "    metrics_combined_loss = np.hstack((metrics_loss[:,1:2], metrics_loss[:,3:4]))         \n",
    "    return model, metrics_class, metrics_localisation, metrics_combined_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5616f12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the relative weighting of the two losses (classificaiton and localisation) \n",
    "# to contribute to the global loss.  Classification first, localisation second. \n",
    "fc_loss_weights = [0.05, 0.95]              \n",
    "# the number of epochs to train the fully connected network for \n",
    "# (ie. the number of times all the training data are passed through the model)\n",
    "n_epochs_fc = 10   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15fb4ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# good loss to use for classification problems, may need to switch to binary if only two classes though?\n",
    "loss_class = losses.categorical_crossentropy     \n",
    "# loss for localisation\n",
    "loss_loc = losses.mean_squared_error       \n",
    "# adam with Nesterov accelerated gradient\n",
    "opt_used = optimizers.Nadam(clipnorm = 1., clipvalue = 0.5)  \n",
    "# accuracy is useful to have on the terminal during training\n",
    "vgg16_2head_fc.compile(optimizer = opt_used, loss=[loss_class, loss_loc], \n",
    "                       # compile the model\n",
    "                       loss_weights = fc_loss_weights, metrics=['accuracy'])    \n",
    "\n",
    "\n",
    "[vgg16_2head_fc,  metrics_class_fc, \n",
    " metrics_localisation_fc, metrics_combined_loss_fc ] = train_double_network(vgg16_2head_fc, \n",
    "                                                                            bottleneck_files_train,\n",
    "                                                                            n_epochs_fc, \n",
    "                                                                            ['class_dense3_loss', \n",
    "                                                                             'loc_dense6_loss'],\n",
    "                                                                            X_validate_btln, \n",
    "                                                                            Y_class_validate, \n",
    "                                                                            Y_loc_validate,\n",
    "                                                                len(synthetic_ifgs_settings['defo_sources']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7ecb27",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #ccffcc; padding: 10px;\">\n",
    "\n",
    "<h1> Plot the training history and  Test the model </h1>\n",
    "\n",
    "`custom_training_history` takes the output metics (accuracy) and plots for classification and localisation so we can see how the accuracy improved over epochs\n",
    "\n",
    "```python\n",
    "Y_class_test_cnn, Y_loc_test_cnn = vgg16_2head_fc.predict(X_test_btln, verbose = 1)  \n",
    "```\n",
    "Generates the prediction labels which are given to our plotting function `plot_data_class_loc_caller`\n",
    "    \n",
    "To show actual and predicted classification and localisation of deformation from the interferograms.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3976c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_training_history(metrics, n_epochs, title = None):\n",
    "    \"\"\"Plot training line graphs for loss and accuracy.  Loss on the left, accuracy on the right.  \n",
    "    Inputs\n",
    "        metrics | r2 array | (n_files * n_epochs) x 2 or 4 matrix,  train loss|validate loss | \n",
    "        train accuracy|validate accuracy. \n",
    "        If no accuracy, only 2 columns\n",
    "        n_epochs | int | number of epochs model was trained for\n",
    "        title | string | title\n",
    "    Returns:\n",
    "        Figure\n",
    "    \"\"\"\n",
    "   \n",
    "    \n",
    "    if metrics.shape[1] == 4:        # detemrine if we have accuracy as well as loss\n",
    "        accuracy_flag = True\n",
    "    else:\n",
    "        accuracy_flag = False\n",
    "        \n",
    "    \n",
    "    n_files = metrics.shape[0] / n_epochs\n",
    "    # Figure output\n",
    "    fig1, axes = plt.subplots(1,2)\n",
    "    fig1.canvas.set_window_title(title)\n",
    "    fig1.suptitle(title)\n",
    "    xvals = np.arange(0,metrics.shape[0])\n",
    "    # fewer validation data; find which ones to plot\n",
    "    validation_plot = np.ravel(np.argwhere(metrics[:,1] > 1e-10))                       \n",
    "    axes[0].plot(xvals, metrics[:,0], c = 'k')                                       # training loss\n",
    "    axes[0].plot(xvals[validation_plot], metrics[validation_plot,1], c = 'r')        # validation loss\n",
    "    axes[0].set_ylabel('Loss')\n",
    "    axes[0].legend(['train', 'validate'], loc='upper left')\n",
    "    axes[0].axhline(y=0, color='k', alpha=0.5)\n",
    "    \n",
    "    if accuracy_flag:\n",
    "        axes[1].plot(xvals, metrics[:,2], c = 'k')                                       # training accuracy\n",
    "        axes[1].plot(xvals[validation_plot], metrics[validation_plot,3], c = 'r')        # validation accuracy\n",
    "        axes[1].set_ylim([0,1])\n",
    "        axes[1].set_ylabel('Accuracy')\n",
    "        axes[1].yaxis.tick_right()\n",
    "        axes[1].legend(['train', 'validate'], loc='upper right')\n",
    "        \n",
    "    #\n",
    "    titles = ['Training loss', 'Training accuracy']\n",
    "    for i in range(2):\n",
    "        axes[i].set_title(titles[i])\n",
    "        # change so a tick only after each epoch (and not each file)\n",
    "        axes[i].set_xticks(np.arange(0,metrics.shape[0],2*n_files))     \n",
    "        axes[i].set_xticklabels(np.arange(0,n_epochs, 2))   # number ticks\n",
    "        axes[i].set_xlabel('Epoch number')\n",
    "\n",
    "    if not accuracy_flag:\n",
    "        axes[1].set_visible(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab3fed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot of the training process for classification\n",
    "custom_training_history(metrics_class_fc, n_epochs_fc, title = 'Fully connected classification training')       \n",
    "# plot of the training process for localisation\n",
    "custom_training_history(metrics_localisation_fc, n_epochs_fc, title = 'Fully connected localisation training')   \n",
    "# save the weights of the model we have trained\n",
    "vgg16_2head_fc.save_weights(f'data/train_fully_connected_model/vgg16_2head_fc.h5')   \n",
    "\n",
    "\n",
    "# Test the model\n",
    "# forward pass of the testing data bottleneck features through the fully connected part of the model\n",
    "Y_class_test_cnn, Y_loc_test_cnn = vgg16_2head_fc.predict(X_test_btln, verbose = 1)                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32729ef3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_data_class_loc_caller(X_test, classes = Y_class_test, classes_predicted = Y_class_test_cnn,       \n",
    "                           # plot all the testing data\n",
    "                           locs = Y_loc_test, locs_predicted = Y_loc_test_cnn, \n",
    "                           source_names = synthetic_ifgs_settings['defo_sources'], \n",
    "                           window_title = 'Testing data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91150d3",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #ccffcc; padding: 10px;\">\n",
    "\n",
    "<h1> Fine Tuning </h1>\n",
    "    \n",
    "The model performs reasonably well but the above plot shows it could do with some fine-tuning\n",
    "\n",
    "1. The models overall loss is now a combination of the classification and localisation loss, which must be balanced using a hyperparameter commonly termed loss weighting. Experimenting with this found that a  value of  0.95  for the classification loss and  0.05  for the localisation loss provided a  good balance between the two outputs as the localisation loss is significantly larger than the classification loss.\n",
    "    \n",
    "```python\n",
    "block5_loss_weights = [0.05, 0.95] \n",
    "```\n",
    "\n",
    "2. As a new style of training is occurring after the 10th epoch the learning rate needs to be carefully selected (too quick and the fine-tuning in both the  convolutional  blocks  of  VGG16 and  our  fully  connected  classification and localisation heads can get destroyed\n",
    "     \n",
    "    \n",
    "```python\n",
    "block5_lr = 1.0e-6  \n",
    "```\n",
    "\n",
    "3. Swithcing the optimizer to stochastic gradient descent (SGD)\n",
    "```python\n",
    "block5_optimiser = optimizers.SGD(lr=block5_lr, momentum=0.9) \n",
    "```\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "<hr>\n",
    "\n",
    "<div style=\"background-color: #ffcdcc; padding: 10px;\">    \n",
    "\n",
    "**This might take a while**   \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba6b5bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Fine-tune the 5th block and the fully connected part of the network):\n",
    "\n",
    "# as per fc_loss_weights, but by changing these more emphasis can be placed on either the clasification \n",
    "# or localisation loss.  \n",
    "block5_loss_weights = [0.05, 0.95]  \n",
    "\n",
    "# We have to set a learning rate manually as an adaptive approach (e.g. NADAM) will be high initially,\n",
    "# and therefore make large updates that will wreck the model (as we're just fine-tuning a model so have \n",
    "# something good to start with)\n",
    "block5_lr = 1.5e-8 \n",
    "\n",
    "# the number of epochs to fine-tune for \n",
    "# (ie. the number of times all the training data are passed through the model)\n",
    "n_epochs_block5 = 10                \n",
    "   \n",
    "np.random.seed(0)                   # 0 used in the example\n",
    "              \n",
    "#%% Fine-tune the 5th convolutional block and the fully connected network.  \n",
    "\n",
    "# VGG16 is used for its convolutional layers and weights (but no fully connected part as we define out own )\n",
    "vgg16_block_1to5 = VGG16(weights='imagenet', include_top=False, input_shape = (224,224,3))  \n",
    "# build the fully connected part of the model, and get the two model outputs\n",
    "output_class, output_loc = define_two_head_model(vgg16_block_1to5.output,\n",
    "                                                 len(synthetic_ifgs_settings['defo_sources']))        \n",
    "\n",
    "\n",
    "vgg16_2head = Model(inputs=vgg16_block_1to5.input, outputs=[output_class, output_loc])                                           # define the full model\n",
    "vgg16_2head.load_weights(f'data/train_fully_connected_model/vgg16_2head_fc.h5', by_name = True)                               # load the weights for the fully connected part which were trained in step 06 (by_name flag so that it doesn't matter that the models are different sizes))\n",
    "\n",
    "for layer in vgg16_2head.layers[:15]:                                                                                             # freeze blocks 1-4 (ie, we are only fine tuneing the 5th block and the fully connected part of the network)\n",
    "    layer.trainable = False    \n",
    "\n",
    "                                      \n",
    "# set the optimizer used in this training part. \n",
    "# Note have to set a learning rate manualy as an adaptive one (eg Nadam) \n",
    "# would wreck model weights in the first few passes before it reduced.         \n",
    "block5_optimiser = optimizers.SGD(lr=block5_lr, momentum=0.9)   \n",
    " \n",
    "vgg16_2head.compile(optimizer = block5_optimiser, metrics=['accuracy'],    \n",
    "                    # recompile as we've changed which layers can be trained/ optimizer etc.  \n",
    "                    loss=[loss_class, loss_loc], loss_weights = block5_loss_weights)                                  \n",
    "\n",
    "try:\n",
    "    plot_model(vgg16_2head, to_file='vgg16_2head.png', show_shapes = True, show_layer_names = True)    \n",
    "    # try to make a graphviz style image showing the complete model \n",
    "except:\n",
    "    print(f\"Failed to create a .png of the model, but continuing anyway.  \")   \n",
    "    vgg16_2head_fc.summary()\n",
    "    # this can easily fail, however, so simply alert the user and continue.  \n",
    "\n",
    "\n",
    "print('\\n\\nFine-tuning the 5th convolutional block and the fully connected network.')\n",
    "\n",
    "[vgg16_2head, metrics_class_5th,\n",
    " metrics_localisation_5th, metrics_combined_loss_5th] = train_double_network(vgg16_2head, data_files_train,\n",
    "                                                      n_epochs_block5, ['class_dense3_loss', 'loc_dense6_loss'],\n",
    "                                                      X_validate, Y_class_validate, Y_loc_validate, \n",
    "                                                      len(synthetic_ifgs_settings['defo_sources']))\n",
    "custom_training_history(metrics_class_5th, n_epochs_block5, title = '5th block classification training')\n",
    "custom_training_history(metrics_localisation_5th, n_epochs_block5, title = '5th block localisation training')\n",
    "try:\n",
    "    os.mkdir((Path(f\"./data/train_full_model\")))\n",
    "except:\n",
    "    pass \n",
    "vgg16_2head.save(f'data/train_full_model/01_vgg16_2head_block5_trained.h5')\n",
    "np.savez(f'data/train_full_model/training_history.npz', metrics_class_fc = metrics_class_fc,\n",
    "                                                         metrics_localisation_fc = metrics_localisation_fc,\n",
    "                                                         metrics_combined_loss_fc = metrics_combined_loss_fc,\n",
    "                                                         metrics_class_5th = metrics_class_5th,\n",
    "                                                         metrics_localisation_5th = metrics_localisation_5th,\n",
    "                                                         metrics_combined_loss_5th = metrics_combined_loss_5th)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08003b6f",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #ccffcc; padding: 10px;\">\n",
    "\n",
    "<h1> Plot the results of the fine tuned model  </h1>\n",
    "\n",
    "\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2db89b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#%% Test with synthetic and real data\n",
    "\n",
    "print('\\n Forward pass of the testing data through the network:')\n",
    "\n",
    "Y_class_test_cnn, Y_loc_test_cnn = vgg16_2head.predict(X_test[:,:,:,:], verbose = 1)                                    # predict class labels\n",
    "\n",
    "\n",
    "plot_data_class_loc_caller(X_test, classes = Y_class_test, classes_predicted = Y_class_test_cnn,                    # plot all the testing data\n",
    "                           locs = Y_loc_test, locs_predicted = Y_loc_test_cnn, \n",
    "                           source_names = synthetic_ifgs_settings['defo_sources'], \n",
    "                           window_title = 'Testing data (after fine tuning)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96b9351",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581c6847",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8190561f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
